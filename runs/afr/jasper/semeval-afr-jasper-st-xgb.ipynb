{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\n\nos.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n\nos.environ['WANDB_DISABLED'] = \"true\"","metadata":{"id":"rVRSZaZ7DdeQ","trusted":true,"execution":{"iopub.status.busy":"2025-01-08T06:26:02.395917Z","iopub.execute_input":"2025-01-08T06:26:02.396196Z","iopub.status.idle":"2025-01-08T06:26:02.400157Z","shell.execute_reply.started":"2025-01-08T06:26:02.396174Z","shell.execute_reply":"2025-01-08T06:26:02.399203Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# !gdown 1qq5gDuEAqWnoviRBDYfbprLFD-MPQyNo\n# !gdown 1mq9CWT9MXsH1Y-ihxVL4uEzufQjpBoer\n!gdown 1EI12smTmdi4Mlu_lLNULN6p6g9dYEWql","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2025-01-08T06:26:02.402228Z","iopub.execute_input":"2025-01-08T06:26:02.402548Z","iopub.status.idle":"2025-01-08T06:26:06.859559Z","shell.execute_reply.started":"2025-01-08T06:26:02.402523Z","shell.execute_reply":"2025-01-08T06:26:06.858694Z"},"id":"jiRg5MiCozSJ","outputId":"f88cfd9b-4fd6-45f1-cda6-92eb5d2bd5a3","trusted":true},"outputs":[{"name":"stdout","text":"Downloading...\nFrom: https://drive.google.com/uc?id=1EI12smTmdi4Mlu_lLNULN6p6g9dYEWql\nTo: /kaggle/working/processed_data.zip\n100%|██████████████████████████████████████| 5.62M/5.62M [00:00<00:00, 31.2MB/s]\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!rm -rf processed_data\n\n!unzip -q processed_data.zip","metadata":{"execution":{"iopub.status.busy":"2025-01-08T06:26:06.860840Z","iopub.execute_input":"2025-01-08T06:26:06.861076Z","iopub.status.idle":"2025-01-08T06:26:07.245132Z","shell.execute_reply.started":"2025-01-08T06:26:06.861056Z","shell.execute_reply":"2025-01-08T06:26:07.244181Z"},"id":"kexrENAEo54E","trusted":true},"outputs":[],"execution_count":3},{"cell_type":"code","source":"!pip install -q transformers[torch] accelerate -U\n\n!pip install -q datasets\n\n!pip install -q transformers\n\n# !pip install -q iterative-stratification\n\n!pip install -q auto-gptq optimum bitsandbytes","metadata":{"execution":{"iopub.status.busy":"2025-01-08T06:26:07.246966Z","iopub.execute_input":"2025-01-08T06:26:07.247208Z","iopub.status.idle":"2025-01-08T06:26:37.987843Z","shell.execute_reply.started":"2025-01-08T06:26:07.247187Z","shell.execute_reply":"2025-01-08T06:26:37.986969Z"},"id":"VC2TifwamSFK","trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"outputId":"a166fedc-051d-4bfb-a183-15ceb57ab4fd"},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.1/44.1 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m336.4/336.4 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m63.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m103.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.5/23.5 MB\u001b[0m \u001b[31m76.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m424.1/424.1 kB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.1/69.1 MB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.8/374.8 kB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m450.7/450.7 kB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.2/13.2 MB\u001b[0m \u001b[31m104.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import numpy as np\n\nimport pandas as pd\n\nimport os\n\nfrom os import path\n\nimport glob\n\nfrom tqdm.notebook import tqdm\n\nfrom tqdm import tqdm\n\nimport string\n\nimport copy\n\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer,IntervalStrategy,BitsAndBytesConfig\n\nfrom datasets import Dataset,load_dataset, DatasetDict\n\n# from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n\n\n\nfrom sklearn.model_selection import train_test_split,KFold\n\nfrom sklearn.metrics import classification_report,accuracy_score, f1_score\n\nfrom sklearn.metrics import precision_recall_fscore_support\n\nfrom sklearn.utils import shuffle\n\nfrom sklearn.preprocessing import LabelEncoder\n\nfrom scipy.special import expit\n\nfrom peft import prepare_model_for_kbit_training\n\nfrom peft import LoraConfig, get_peft_model,PeftModel\n","metadata":{"execution":{"iopub.status.busy":"2025-01-08T06:26:37.989209Z","iopub.execute_input":"2025-01-08T06:26:37.989460Z","iopub.status.idle":"2025-01-08T06:26:52.482316Z","shell.execute_reply.started":"2025-01-08T06:26:37.989413Z","shell.execute_reply":"2025-01-08T06:26:52.481558Z"},"id":"a_v9ZQjVoIma","trusted":true},"outputs":[],"execution_count":5},{"cell_type":"code","source":"import torch\n\nimport torch.nn as nn\n\nimport torch.nn.functional as F","metadata":{"execution":{"iopub.status.busy":"2025-01-08T06:26:52.483358Z","iopub.execute_input":"2025-01-08T06:26:52.483940Z","iopub.status.idle":"2025-01-08T06:26:52.487526Z","shell.execute_reply.started":"2025-01-08T06:26:52.483914Z","shell.execute_reply":"2025-01-08T06:26:52.486728Z"},"id":"BN44kbQ4ocG2","trusted":true},"outputs":[],"execution_count":6},{"cell_type":"code","source":"\n\nmodels_list_3=[\n        # \"FacebookAI/xlm-roberta-large\",\n      # \"google-bert/bert-base-uncased\",\n        \"infgrad/jasper_en_vision_language_v1\",\n\n#      \"google-bert/bert-base-multilingual-uncased\",\n\n          ]\n\n\n\nmodels_name_3=[\n# \"FacebookAI/xlm-roberta-large\",\n# \"google-bert/bert-base-uncased\",\n        \"infgrad/jasper_en_vision_language_v1\",\n\n# \"bert\",\n\n]\n","metadata":{"execution":{"iopub.status.busy":"2025-01-08T06:26:52.488302Z","iopub.execute_input":"2025-01-08T06:26:52.488590Z","iopub.status.idle":"2025-01-08T06:26:52.649589Z","shell.execute_reply.started":"2025-01-08T06:26:52.488553Z","shell.execute_reply":"2025-01-08T06:26:52.648786Z"},"id":"Ugv0OfRImSin","trusted":true},"outputs":[],"execution_count":7},{"cell_type":"code","source":"kind=\"train\"\n\ntask=\"track_a\"\n\n# langs=[\"afr\",\"amh\",\"deu\",\"eng\",\"oro\",\"ptbr\",\"rus\",\"som\",\"sum\",\"tir\"]\n# labels=['Anger','Disgust', 'Fear', 'Joy', 'Sadness', 'Surprise']\n# langs=[\"amh\",\"arq\",\"ary\",\"chn\",\"deu\",\"esp\",\"hau\",\"hin\",\"ibo\",\"kin\",\"mar\",\"orm\",\"pcm\",\"ptbr\",\"ptmz\",\"ron\",\"rus\",\"som\",\"sun\",\"swa\",\"swe\",\"tat\",\"tir\",\"ukr\",\"vmw\",\"yor\",\"ind\",\"jav\",\"xho\",\"zul\"]\n# langs=[\"eng\"]\nlangs=[\"afr\"]\n\n\n# processed_path=f\"processed_data/{kinds[0]}/{tasks[0]}/{langs[3]}.csv\"\n\n\n\n# train_data=pd.read_csv(processed_path)\n","metadata":{"execution":{"iopub.status.busy":"2025-01-08T06:26:52.650529Z","iopub.execute_input":"2025-01-08T06:26:52.650803Z","iopub.status.idle":"2025-01-08T06:26:52.665067Z","shell.execute_reply.started":"2025-01-08T06:26:52.650779Z","shell.execute_reply":"2025-01-08T06:26:52.664236Z"},"id":"1kwye6PzpT4N","trusted":true},"outputs":[],"execution_count":8},{"cell_type":"code","source":"all_csv=pd.DataFrame()\n\nfor lang in langs:\n\n    processed_path=f\"processed_data/{kind}/{task}/{lang}.csv\"\n    # if kind==\"dev\":\n    #    processed_path=processed_path.replace(lang,lang+task[-2:])\n    if not os.path.isfile(processed_path):\n\n      print(\"not found:\",processed_path)\n\n      continue\n    train_data=pd.read_csv(processed_path)\n\n    train_data.columns = train_data.columns.str.lower()\n\n    all_csv = pd.concat([all_csv, train_data],ignore_index=True)\n\n\n","metadata":{"execution":{"iopub.status.busy":"2025-01-08T06:26:52.665815Z","iopub.execute_input":"2025-01-08T06:26:52.666031Z","iopub.status.idle":"2025-01-08T06:26:52.698417Z","shell.execute_reply.started":"2025-01-08T06:26:52.666013Z","shell.execute_reply":"2025-01-08T06:26:52.697852Z"},"id":"1x0MboqJuj21","trusted":true},"outputs":[],"execution_count":9},{"cell_type":"code","source":"all_csv","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":423},"execution":{"iopub.status.busy":"2025-01-08T06:26:52.700903Z","iopub.execute_input":"2025-01-08T06:26:52.701119Z","iopub.status.idle":"2025-01-08T06:26:52.719048Z","shell.execute_reply.started":"2025-01-08T06:26:52.701100Z","shell.execute_reply":"2025-01-08T06:26:52.718341Z"},"id":"owuvojrICn5c","outputId":"5a25e14b-29c1-4f56-c09b-fc19df1e4c16","trusted":true},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"                           id  anger  disgust  fear  joy  sadness lang  \\\n0     afr_train_track_a_00001      0        0     0    0        0  afr   \n1     afr_train_track_a_00002      0        0     0    0        0  afr   \n2     afr_train_track_a_00003      0        0     0    0        1  afr   \n3     afr_train_track_a_00004      0        0     0    1        0  afr   \n4     afr_train_track_a_00005      0        0     0    1        0  afr   \n...                       ...    ...      ...   ...  ...      ...  ...   \n1217  afr_train_track_a_01218      1        1     0    0        0  afr   \n1218  afr_train_track_a_01219      0        0     0    1        0  afr   \n1219  afr_train_track_a_01220      0        0     0    1        0  afr   \n1220  afr_train_track_a_01221      0        0     0    1        0  afr   \n1221  afr_train_track_a_01222      0        0     0    1        0  afr   \n\n                                          clean_message  \n0     die grondeienaars het die departement genader ...  \n1     dit is verder n erkende feit dat daar meningsv...  \n2     baie families in die weskaap is in rou gedompe...  \n3          ons wil u deelmaak van die werk wat ons doen  \n4     en dit onderstreep waarom naln en nelm gesamen...  \n...                                                 ...  \n1217  rassisme moet finaal vir eens en vir altyd beg...  \n1218  sy werk hard en verwag vinnige resultate van h...  \n1219  tweehonderd sesensestig kinders van al die pro...  \n1220  onder andere het die weskaapse kultuurkommissi...  \n1221  twee lekker hoogtepunte vir naln was die museu...  \n\n[1222 rows x 8 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>anger</th>\n      <th>disgust</th>\n      <th>fear</th>\n      <th>joy</th>\n      <th>sadness</th>\n      <th>lang</th>\n      <th>clean_message</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>afr_train_track_a_00001</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>afr</td>\n      <td>die grondeienaars het die departement genader ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>afr_train_track_a_00002</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>afr</td>\n      <td>dit is verder n erkende feit dat daar meningsv...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>afr_train_track_a_00003</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>afr</td>\n      <td>baie families in die weskaap is in rou gedompe...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>afr_train_track_a_00004</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>afr</td>\n      <td>ons wil u deelmaak van die werk wat ons doen</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>afr_train_track_a_00005</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>afr</td>\n      <td>en dit onderstreep waarom naln en nelm gesamen...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1217</th>\n      <td>afr_train_track_a_01218</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>afr</td>\n      <td>rassisme moet finaal vir eens en vir altyd beg...</td>\n    </tr>\n    <tr>\n      <th>1218</th>\n      <td>afr_train_track_a_01219</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>afr</td>\n      <td>sy werk hard en verwag vinnige resultate van h...</td>\n    </tr>\n    <tr>\n      <th>1219</th>\n      <td>afr_train_track_a_01220</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>afr</td>\n      <td>tweehonderd sesensestig kinders van al die pro...</td>\n    </tr>\n    <tr>\n      <th>1220</th>\n      <td>afr_train_track_a_01221</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>afr</td>\n      <td>onder andere het die weskaapse kultuurkommissi...</td>\n    </tr>\n    <tr>\n      <th>1221</th>\n      <td>afr_train_track_a_01222</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>afr</td>\n      <td>twee lekker hoogtepunte vir naln was die museu...</td>\n    </tr>\n  </tbody>\n</table>\n<p>1222 rows × 8 columns</p>\n</div>"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"all_csv[all_csv[\"lang\"]==\"eng\"]","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":423},"execution":{"iopub.status.busy":"2025-01-08T06:26:52.720762Z","iopub.execute_input":"2025-01-08T06:26:52.720998Z","iopub.status.idle":"2025-01-08T06:26:52.730349Z","shell.execute_reply.started":"2025-01-08T06:26:52.720979Z","shell.execute_reply":"2025-01-08T06:26:52.729776Z"},"id":"zm9h2f0rBad4","outputId":"4edf94bb-60aa-4aed-bcbc-7e86e1aa4a99","trusted":true},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"Empty DataFrame\nColumns: [id, anger, disgust, fear, joy, sadness, lang, clean_message]\nIndex: []","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>anger</th>\n      <th>disgust</th>\n      <th>fear</th>\n      <th>joy</th>\n      <th>sadness</th>\n      <th>lang</th>\n      <th>clean_message</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"# float_columns = all_csv.select_dtypes(include=['float64']).columns\n# # Check each column and convert only if there are no NaN values\n# for col in float_columns:\n#     if all_csv[col].isna().sum() == 0:  # No NaN values in the column\n#         print(f\"converting {kind} {task} {lang} column {col} to int\")\n#         all_csv[col] = all_csv[col].astype('int')\n#     else:\n#         print(f\"converting {kind} {task} {lang} column {col} to int\")","metadata":{"execution":{"iopub.status.busy":"2025-01-08T06:26:52.731248Z","iopub.execute_input":"2025-01-08T06:26:52.731619Z","iopub.status.idle":"2025-01-08T06:26:52.744037Z","shell.execute_reply.started":"2025-01-08T06:26:52.731590Z","shell.execute_reply":"2025-01-08T06:26:52.743244Z"},"id":"BDrtrsvCBad4","trusted":true},"outputs":[],"execution_count":12},{"cell_type":"code","source":"train_data=all_csv\n","metadata":{"execution":{"iopub.status.busy":"2025-01-08T06:26:52.744945Z","iopub.execute_input":"2025-01-08T06:26:52.745263Z","iopub.status.idle":"2025-01-08T06:26:52.757455Z","shell.execute_reply.started":"2025-01-08T06:26:52.745232Z","shell.execute_reply":"2025-01-08T06:26:52.756817Z"},"id":"zITfHiVrBad5","trusted":true},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# train_data = train_data.drop(columns=['disgust'],axis=1)\n","metadata":{"execution":{"iopub.status.busy":"2025-01-08T06:26:52.758184Z","iopub.execute_input":"2025-01-08T06:26:52.758460Z","iopub.status.idle":"2025-01-08T06:26:52.770887Z","shell.execute_reply.started":"2025-01-08T06:26:52.758415Z","shell.execute_reply":"2025-01-08T06:26:52.770074Z"},"id":"kN_EpabwBad5","trusted":true},"outputs":[],"execution_count":14},{"cell_type":"code","source":"\ntrain_data=train_data.fillna(0)\n\ntrain_data","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":423},"execution":{"iopub.status.busy":"2025-01-08T06:26:52.771871Z","iopub.execute_input":"2025-01-08T06:26:52.772144Z","iopub.status.idle":"2025-01-08T06:26:52.792786Z","shell.execute_reply.started":"2025-01-08T06:26:52.772115Z","shell.execute_reply":"2025-01-08T06:26:52.791975Z"},"id":"7a-08HGownlu","outputId":"a4cfc4ff-c530-46ef-fd44-9f48559bdd39","trusted":true},"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"                           id  anger  disgust  fear  joy  sadness lang  \\\n0     afr_train_track_a_00001      0        0     0    0        0  afr   \n1     afr_train_track_a_00002      0        0     0    0        0  afr   \n2     afr_train_track_a_00003      0        0     0    0        1  afr   \n3     afr_train_track_a_00004      0        0     0    1        0  afr   \n4     afr_train_track_a_00005      0        0     0    1        0  afr   \n...                       ...    ...      ...   ...  ...      ...  ...   \n1217  afr_train_track_a_01218      1        1     0    0        0  afr   \n1218  afr_train_track_a_01219      0        0     0    1        0  afr   \n1219  afr_train_track_a_01220      0        0     0    1        0  afr   \n1220  afr_train_track_a_01221      0        0     0    1        0  afr   \n1221  afr_train_track_a_01222      0        0     0    1        0  afr   \n\n                                          clean_message  \n0     die grondeienaars het die departement genader ...  \n1     dit is verder n erkende feit dat daar meningsv...  \n2     baie families in die weskaap is in rou gedompe...  \n3          ons wil u deelmaak van die werk wat ons doen  \n4     en dit onderstreep waarom naln en nelm gesamen...  \n...                                                 ...  \n1217  rassisme moet finaal vir eens en vir altyd beg...  \n1218  sy werk hard en verwag vinnige resultate van h...  \n1219  tweehonderd sesensestig kinders van al die pro...  \n1220  onder andere het die weskaapse kultuurkommissi...  \n1221  twee lekker hoogtepunte vir naln was die museu...  \n\n[1222 rows x 8 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>anger</th>\n      <th>disgust</th>\n      <th>fear</th>\n      <th>joy</th>\n      <th>sadness</th>\n      <th>lang</th>\n      <th>clean_message</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>afr_train_track_a_00001</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>afr</td>\n      <td>die grondeienaars het die departement genader ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>afr_train_track_a_00002</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>afr</td>\n      <td>dit is verder n erkende feit dat daar meningsv...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>afr_train_track_a_00003</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>afr</td>\n      <td>baie families in die weskaap is in rou gedompe...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>afr_train_track_a_00004</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>afr</td>\n      <td>ons wil u deelmaak van die werk wat ons doen</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>afr_train_track_a_00005</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>afr</td>\n      <td>en dit onderstreep waarom naln en nelm gesamen...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1217</th>\n      <td>afr_train_track_a_01218</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>afr</td>\n      <td>rassisme moet finaal vir eens en vir altyd beg...</td>\n    </tr>\n    <tr>\n      <th>1218</th>\n      <td>afr_train_track_a_01219</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>afr</td>\n      <td>sy werk hard en verwag vinnige resultate van h...</td>\n    </tr>\n    <tr>\n      <th>1219</th>\n      <td>afr_train_track_a_01220</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>afr</td>\n      <td>tweehonderd sesensestig kinders van al die pro...</td>\n    </tr>\n    <tr>\n      <th>1220</th>\n      <td>afr_train_track_a_01221</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>afr</td>\n      <td>onder andere het die weskaapse kultuurkommissi...</td>\n    </tr>\n    <tr>\n      <th>1221</th>\n      <td>afr_train_track_a_01222</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>afr</td>\n      <td>twee lekker hoogtepunte vir naln was die museu...</td>\n    </tr>\n  </tbody>\n</table>\n<p>1222 rows × 8 columns</p>\n</div>"},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"float_columns = train_data.select_dtypes(include=['float64']).columns\n\n# Convert those columns to int\ntrain_data[float_columns] = train_data[float_columns].astype('int')\ntrain_data","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":423},"execution":{"iopub.status.busy":"2025-01-08T06:26:52.793659Z","iopub.execute_input":"2025-01-08T06:26:52.793954Z","iopub.status.idle":"2025-01-08T06:26:52.816785Z","shell.execute_reply.started":"2025-01-08T06:26:52.793923Z","shell.execute_reply":"2025-01-08T06:26:52.816040Z"},"id":"wWyOiTLLBad5","outputId":"f123ea84-90bb-4b08-ef33-32b52741c8c4","trusted":true},"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"                           id  anger  disgust  fear  joy  sadness lang  \\\n0     afr_train_track_a_00001      0        0     0    0        0  afr   \n1     afr_train_track_a_00002      0        0     0    0        0  afr   \n2     afr_train_track_a_00003      0        0     0    0        1  afr   \n3     afr_train_track_a_00004      0        0     0    1        0  afr   \n4     afr_train_track_a_00005      0        0     0    1        0  afr   \n...                       ...    ...      ...   ...  ...      ...  ...   \n1217  afr_train_track_a_01218      1        1     0    0        0  afr   \n1218  afr_train_track_a_01219      0        0     0    1        0  afr   \n1219  afr_train_track_a_01220      0        0     0    1        0  afr   \n1220  afr_train_track_a_01221      0        0     0    1        0  afr   \n1221  afr_train_track_a_01222      0        0     0    1        0  afr   \n\n                                          clean_message  \n0     die grondeienaars het die departement genader ...  \n1     dit is verder n erkende feit dat daar meningsv...  \n2     baie families in die weskaap is in rou gedompe...  \n3          ons wil u deelmaak van die werk wat ons doen  \n4     en dit onderstreep waarom naln en nelm gesamen...  \n...                                                 ...  \n1217  rassisme moet finaal vir eens en vir altyd beg...  \n1218  sy werk hard en verwag vinnige resultate van h...  \n1219  tweehonderd sesensestig kinders van al die pro...  \n1220  onder andere het die weskaapse kultuurkommissi...  \n1221  twee lekker hoogtepunte vir naln was die museu...  \n\n[1222 rows x 8 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>anger</th>\n      <th>disgust</th>\n      <th>fear</th>\n      <th>joy</th>\n      <th>sadness</th>\n      <th>lang</th>\n      <th>clean_message</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>afr_train_track_a_00001</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>afr</td>\n      <td>die grondeienaars het die departement genader ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>afr_train_track_a_00002</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>afr</td>\n      <td>dit is verder n erkende feit dat daar meningsv...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>afr_train_track_a_00003</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>afr</td>\n      <td>baie families in die weskaap is in rou gedompe...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>afr_train_track_a_00004</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>afr</td>\n      <td>ons wil u deelmaak van die werk wat ons doen</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>afr_train_track_a_00005</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>afr</td>\n      <td>en dit onderstreep waarom naln en nelm gesamen...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1217</th>\n      <td>afr_train_track_a_01218</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>afr</td>\n      <td>rassisme moet finaal vir eens en vir altyd beg...</td>\n    </tr>\n    <tr>\n      <th>1218</th>\n      <td>afr_train_track_a_01219</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>afr</td>\n      <td>sy werk hard en verwag vinnige resultate van h...</td>\n    </tr>\n    <tr>\n      <th>1219</th>\n      <td>afr_train_track_a_01220</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>afr</td>\n      <td>tweehonderd sesensestig kinders van al die pro...</td>\n    </tr>\n    <tr>\n      <th>1220</th>\n      <td>afr_train_track_a_01221</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>afr</td>\n      <td>onder andere het die weskaapse kultuurkommissi...</td>\n    </tr>\n    <tr>\n      <th>1221</th>\n      <td>afr_train_track_a_01222</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>afr</td>\n      <td>twee lekker hoogtepunte vir naln was die museu...</td>\n    </tr>\n  </tbody>\n</table>\n<p>1222 rows × 8 columns</p>\n</div>"},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"train_data.dtypes","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":335},"execution":{"iopub.status.busy":"2025-01-08T06:26:52.817543Z","iopub.execute_input":"2025-01-08T06:26:52.817827Z","iopub.status.idle":"2025-01-08T06:26:52.833547Z","shell.execute_reply.started":"2025-01-08T06:26:52.817805Z","shell.execute_reply":"2025-01-08T06:26:52.832870Z"},"id":"LfElY7blBad5","outputId":"c7420084-c035-48e3-db2b-b8c6a8806f0a","trusted":true},"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"id               object\nanger             int64\ndisgust           int64\nfear              int64\njoy               int64\nsadness           int64\nlang             object\nclean_message    object\ndtype: object"},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"train_data = train_data[\n\ntrain_data['clean_message'].isnull()==False]\n\ntrain_data.reset_index()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":423},"execution":{"iopub.status.busy":"2025-01-08T06:26:52.834319Z","iopub.execute_input":"2025-01-08T06:26:52.834595Z","iopub.status.idle":"2025-01-08T06:26:52.859069Z","shell.execute_reply.started":"2025-01-08T06:26:52.834574Z","shell.execute_reply":"2025-01-08T06:26:52.858187Z"},"id":"tlabsablmWzT","outputId":"5fea7408-28f5-4f04-9631-e30efec85fc1","trusted":true},"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"      index                       id  anger  disgust  fear  joy  sadness lang  \\\n0         0  afr_train_track_a_00001      0        0     0    0        0  afr   \n1         1  afr_train_track_a_00002      0        0     0    0        0  afr   \n2         2  afr_train_track_a_00003      0        0     0    0        1  afr   \n3         3  afr_train_track_a_00004      0        0     0    1        0  afr   \n4         4  afr_train_track_a_00005      0        0     0    1        0  afr   \n...     ...                      ...    ...      ...   ...  ...      ...  ...   \n1217   1217  afr_train_track_a_01218      1        1     0    0        0  afr   \n1218   1218  afr_train_track_a_01219      0        0     0    1        0  afr   \n1219   1219  afr_train_track_a_01220      0        0     0    1        0  afr   \n1220   1220  afr_train_track_a_01221      0        0     0    1        0  afr   \n1221   1221  afr_train_track_a_01222      0        0     0    1        0  afr   \n\n                                          clean_message  \n0     die grondeienaars het die departement genader ...  \n1     dit is verder n erkende feit dat daar meningsv...  \n2     baie families in die weskaap is in rou gedompe...  \n3          ons wil u deelmaak van die werk wat ons doen  \n4     en dit onderstreep waarom naln en nelm gesamen...  \n...                                                 ...  \n1217  rassisme moet finaal vir eens en vir altyd beg...  \n1218  sy werk hard en verwag vinnige resultate van h...  \n1219  tweehonderd sesensestig kinders van al die pro...  \n1220  onder andere het die weskaapse kultuurkommissi...  \n1221  twee lekker hoogtepunte vir naln was die museu...  \n\n[1222 rows x 9 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>index</th>\n      <th>id</th>\n      <th>anger</th>\n      <th>disgust</th>\n      <th>fear</th>\n      <th>joy</th>\n      <th>sadness</th>\n      <th>lang</th>\n      <th>clean_message</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>afr_train_track_a_00001</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>afr</td>\n      <td>die grondeienaars het die departement genader ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>afr_train_track_a_00002</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>afr</td>\n      <td>dit is verder n erkende feit dat daar meningsv...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>afr_train_track_a_00003</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>afr</td>\n      <td>baie families in die weskaap is in rou gedompe...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>afr_train_track_a_00004</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>afr</td>\n      <td>ons wil u deelmaak van die werk wat ons doen</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>afr_train_track_a_00005</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>afr</td>\n      <td>en dit onderstreep waarom naln en nelm gesamen...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1217</th>\n      <td>1217</td>\n      <td>afr_train_track_a_01218</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>afr</td>\n      <td>rassisme moet finaal vir eens en vir altyd beg...</td>\n    </tr>\n    <tr>\n      <th>1218</th>\n      <td>1218</td>\n      <td>afr_train_track_a_01219</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>afr</td>\n      <td>sy werk hard en verwag vinnige resultate van h...</td>\n    </tr>\n    <tr>\n      <th>1219</th>\n      <td>1219</td>\n      <td>afr_train_track_a_01220</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>afr</td>\n      <td>tweehonderd sesensestig kinders van al die pro...</td>\n    </tr>\n    <tr>\n      <th>1220</th>\n      <td>1220</td>\n      <td>afr_train_track_a_01221</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>afr</td>\n      <td>onder andere het die weskaapse kultuurkommissi...</td>\n    </tr>\n    <tr>\n      <th>1221</th>\n      <td>1221</td>\n      <td>afr_train_track_a_01222</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>afr</td>\n      <td>twee lekker hoogtepunte vir naln was die museu...</td>\n    </tr>\n  </tbody>\n</table>\n<p>1222 rows × 9 columns</p>\n</div>"},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"train_data","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":423},"execution":{"iopub.status.busy":"2025-01-08T06:26:52.860549Z","iopub.execute_input":"2025-01-08T06:26:52.860748Z","iopub.status.idle":"2025-01-08T06:26:52.880252Z","shell.execute_reply.started":"2025-01-08T06:26:52.860730Z","shell.execute_reply":"2025-01-08T06:26:52.879675Z"},"id":"3sl4zq2jBad6","outputId":"3ca5e8cd-2362-4bc6-841a-fb10a8d58bb1","trusted":true},"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"                           id  anger  disgust  fear  joy  sadness lang  \\\n0     afr_train_track_a_00001      0        0     0    0        0  afr   \n1     afr_train_track_a_00002      0        0     0    0        0  afr   \n2     afr_train_track_a_00003      0        0     0    0        1  afr   \n3     afr_train_track_a_00004      0        0     0    1        0  afr   \n4     afr_train_track_a_00005      0        0     0    1        0  afr   \n...                       ...    ...      ...   ...  ...      ...  ...   \n1217  afr_train_track_a_01218      1        1     0    0        0  afr   \n1218  afr_train_track_a_01219      0        0     0    1        0  afr   \n1219  afr_train_track_a_01220      0        0     0    1        0  afr   \n1220  afr_train_track_a_01221      0        0     0    1        0  afr   \n1221  afr_train_track_a_01222      0        0     0    1        0  afr   \n\n                                          clean_message  \n0     die grondeienaars het die departement genader ...  \n1     dit is verder n erkende feit dat daar meningsv...  \n2     baie families in die weskaap is in rou gedompe...  \n3          ons wil u deelmaak van die werk wat ons doen  \n4     en dit onderstreep waarom naln en nelm gesamen...  \n...                                                 ...  \n1217  rassisme moet finaal vir eens en vir altyd beg...  \n1218  sy werk hard en verwag vinnige resultate van h...  \n1219  tweehonderd sesensestig kinders van al die pro...  \n1220  onder andere het die weskaapse kultuurkommissi...  \n1221  twee lekker hoogtepunte vir naln was die museu...  \n\n[1222 rows x 8 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>anger</th>\n      <th>disgust</th>\n      <th>fear</th>\n      <th>joy</th>\n      <th>sadness</th>\n      <th>lang</th>\n      <th>clean_message</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>afr_train_track_a_00001</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>afr</td>\n      <td>die grondeienaars het die departement genader ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>afr_train_track_a_00002</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>afr</td>\n      <td>dit is verder n erkende feit dat daar meningsv...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>afr_train_track_a_00003</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>afr</td>\n      <td>baie families in die weskaap is in rou gedompe...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>afr_train_track_a_00004</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>afr</td>\n      <td>ons wil u deelmaak van die werk wat ons doen</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>afr_train_track_a_00005</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>afr</td>\n      <td>en dit onderstreep waarom naln en nelm gesamen...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1217</th>\n      <td>afr_train_track_a_01218</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>afr</td>\n      <td>rassisme moet finaal vir eens en vir altyd beg...</td>\n    </tr>\n    <tr>\n      <th>1218</th>\n      <td>afr_train_track_a_01219</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>afr</td>\n      <td>sy werk hard en verwag vinnige resultate van h...</td>\n    </tr>\n    <tr>\n      <th>1219</th>\n      <td>afr_train_track_a_01220</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>afr</td>\n      <td>tweehonderd sesensestig kinders van al die pro...</td>\n    </tr>\n    <tr>\n      <th>1220</th>\n      <td>afr_train_track_a_01221</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>afr</td>\n      <td>onder andere het die weskaapse kultuurkommissi...</td>\n    </tr>\n    <tr>\n      <th>1221</th>\n      <td>afr_train_track_a_01222</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>afr</td>\n      <td>twee lekker hoogtepunte vir naln was die museu...</td>\n    </tr>\n  </tbody>\n</table>\n<p>1222 rows × 8 columns</p>\n</div>"},"metadata":{}}],"execution_count":19},{"cell_type":"code","source":"# label_columns =['anger', 'fear', 'joy', 'sadness', 'surprise'] #eng\nlabel_columns =['anger', 'disgust', 'fear', 'joy', 'sadness'] #afr\n# label_columns =['anger', 'disgust', 'fear', 'joy', 'sadness', 'surprise'] #all\n","metadata":{"execution":{"iopub.status.busy":"2025-01-08T06:26:52.880972Z","iopub.execute_input":"2025-01-08T06:26:52.881171Z","iopub.status.idle":"2025-01-08T06:26:52.895514Z","shell.execute_reply.started":"2025-01-08T06:26:52.881153Z","shell.execute_reply":"2025-01-08T06:26:52.894776Z"},"id":"2RXwzOOkBad6","trusted":true},"outputs":[],"execution_count":20},{"cell_type":"code","source":"train_data['lang'].value_counts()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":147},"execution":{"iopub.status.busy":"2025-01-08T06:26:52.896273Z","iopub.execute_input":"2025-01-08T06:26:52.896578Z","iopub.status.idle":"2025-01-08T06:26:52.920030Z","shell.execute_reply.started":"2025-01-08T06:26:52.896550Z","shell.execute_reply":"2025-01-08T06:26:52.919234Z"},"id":"5SDZ3wEUBad6","outputId":"18168d84-c685-44f1-bd87-753080ce1b86","trusted":true},"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"lang\nafr    1222\nName: count, dtype: int64"},"metadata":{}}],"execution_count":21},{"cell_type":"code","source":"sum(train_data['lang'].isna())","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2025-01-08T06:26:52.920749Z","iopub.execute_input":"2025-01-08T06:26:52.921011Z","iopub.status.idle":"2025-01-08T06:26:52.937091Z","shell.execute_reply.started":"2025-01-08T06:26:52.920979Z","shell.execute_reply":"2025-01-08T06:26:52.936314Z"},"id":"5ubyh_YPBad6","outputId":"cbd2f414-de02-477c-b905-09f5f6c113cc","trusted":true},"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}],"execution_count":22},{"cell_type":"code","source":"nan_rows = train_data[train_data.isna().any(axis=1)]\nnan_rows\n# Group by 'lang' and list the rows with NaNs for each language\n# nan_by_language = nan_rows.groupby('lang').apply(lambda x: x)\n\n# Display the result\n# print(nan_by_language)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":89},"execution":{"iopub.status.busy":"2025-01-08T06:26:52.937941Z","iopub.execute_input":"2025-01-08T06:26:52.938129Z","iopub.status.idle":"2025-01-08T06:26:52.956703Z","shell.execute_reply.started":"2025-01-08T06:26:52.938111Z","shell.execute_reply":"2025-01-08T06:26:52.956012Z"},"id":"8lubfAyYBad6","outputId":"566f09ac-4bc1-47e2-b8cd-8425fa41b14b","trusted":true},"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"Empty DataFrame\nColumns: [id, anger, disgust, fear, joy, sadness, lang, clean_message]\nIndex: []","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>anger</th>\n      <th>disgust</th>\n      <th>fear</th>\n      <th>joy</th>\n      <th>sadness</th>\n      <th>lang</th>\n      <th>clean_message</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":23},{"cell_type":"code","source":"nan_rows[nan_rows[\"lang\"]==\"eng\"]","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":53},"execution":{"iopub.status.busy":"2025-01-08T06:26:52.957598Z","iopub.execute_input":"2025-01-08T06:26:52.957877Z","iopub.status.idle":"2025-01-08T06:26:52.973239Z","shell.execute_reply.started":"2025-01-08T06:26:52.957849Z","shell.execute_reply":"2025-01-08T06:26:52.972629Z"},"id":"egecP3nTBad6","outputId":"e74dac3f-c289-4b45-9eb1-cb607791feed","trusted":true},"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"Empty DataFrame\nColumns: [id, anger, disgust, fear, joy, sadness, lang, clean_message]\nIndex: []","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>anger</th>\n      <th>disgust</th>\n      <th>fear</th>\n      <th>joy</th>\n      <th>sadness</th>\n      <th>lang</th>\n      <th>clean_message</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":24},{"cell_type":"code","source":"train_data[train_data[\"lang\"]==\"deu\"].dtypes","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":335},"execution":{"iopub.status.busy":"2025-01-08T06:26:52.974124Z","iopub.execute_input":"2025-01-08T06:26:52.974330Z","iopub.status.idle":"2025-01-08T06:26:52.988734Z","shell.execute_reply.started":"2025-01-08T06:26:52.974301Z","shell.execute_reply":"2025-01-08T06:26:52.987958Z"},"id":"kweBEYKVBad6","outputId":"ced241cc-ea95-4321-ac1e-90f2f60a6bea","trusted":true},"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"id               object\nanger             int64\ndisgust           int64\nfear              int64\njoy               int64\nsadness           int64\nlang             object\nclean_message    object\ndtype: object"},"metadata":{}}],"execution_count":25},{"cell_type":"code","source":"nan_rows[\"lang\"].value_counts()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":115},"execution":{"iopub.status.busy":"2025-01-08T06:26:52.989527Z","iopub.execute_input":"2025-01-08T06:26:52.989771Z","iopub.status.idle":"2025-01-08T06:26:53.003825Z","shell.execute_reply.started":"2025-01-08T06:26:52.989741Z","shell.execute_reply":"2025-01-08T06:26:53.003006Z"},"id":"dkMYBknIBad7","outputId":"622da8dc-f3f5-4e42-a4fd-87a149811f60","trusted":true},"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"Series([], Name: count, dtype: int64)"},"metadata":{}}],"execution_count":26},{"cell_type":"code","source":"nan_rows.dtypes","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":335},"execution":{"iopub.status.busy":"2025-01-08T06:26:53.004717Z","iopub.execute_input":"2025-01-08T06:26:53.004988Z","iopub.status.idle":"2025-01-08T06:26:53.019276Z","shell.execute_reply.started":"2025-01-08T06:26:53.004958Z","shell.execute_reply":"2025-01-08T06:26:53.018653Z"},"id":"OrpSoZdfBad7","outputId":"22a0d557-7820-4d68-f03d-e2d905277788","trusted":true},"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"id               object\nanger             int64\ndisgust           int64\nfear              int64\njoy               int64\nsadness           int64\nlang             object\nclean_message    object\ndtype: object"},"metadata":{}}],"execution_count":27},{"cell_type":"code","source":"# Step 2: Count how many columns are not 0 or 1 for each language\ndef count_nonzero_nonone_rows(group):\n    # Exclude the 'lang' column from counting if needed\n    # Use boolean conditions to count values that are not 0 or 1\n    return (group.drop(columns='anger') != 0) & (group.drop(columns='anger') != 1).sum(axis=1).sum()\n\n# Group by 'lang' and apply the function to count non-zero, non-one values\nnon_zero_non_one_counts = train_data.groupby('lang').apply(count_nonzero_nonone_rows)\n\n# Display the results\n# result = pd.DataFrame({\n#     'row_count': lang_row_counts,\n#     'non_zero_non_one_count': non_zero_non_one_counts\n# })\n\nprint(non_zero_non_one_counts)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2025-01-08T06:26:53.024011Z","iopub.execute_input":"2025-01-08T06:26:53.024232Z","iopub.status.idle":"2025-01-08T06:26:53.043983Z","shell.execute_reply.started":"2025-01-08T06:26:53.024212Z","shell.execute_reply":"2025-01-08T06:26:53.043161Z"},"id":"a1FDeJieBad7","outputId":"b38619b7-3d57-407f-fbd5-7265fc6fffc8","trusted":true},"outputs":[{"name":"stdout","text":"             id  disgust   fear    joy  sadness  lang  clean_message\nlang                                                                \nafr  0     True    False  False  False    False  True           True\n     1     True    False  False  False    False  True           True\n     2     True    False  False  False     True  True           True\n     3     True    False  False   True    False  True           True\n     4     True    False  False   True    False  True           True\n...         ...      ...    ...    ...      ...   ...            ...\n     1217  True     True  False  False    False  True           True\n     1218  True    False  False   True    False  True           True\n     1219  True    False  False   True    False  True           True\n     1220  True    False  False   True    False  True           True\n     1221  True    False  False   True    False  True           True\n\n[1222 rows x 7 columns]\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"maxi=train_data['clean_message'].apply(len).max()\nmaxi","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VPAf2dnTZF_H","outputId":"eeedca24-5588-4617-ee63-14b41529cef8","trusted":true,"execution":{"iopub.status.busy":"2025-01-08T06:26:53.045784Z","iopub.execute_input":"2025-01-08T06:26:53.045998Z","iopub.status.idle":"2025-01-08T06:26:53.057082Z","shell.execute_reply.started":"2025-01-08T06:26:53.045979Z","shell.execute_reply":"2025-01-08T06:26:53.056339Z"}},"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"176"},"metadata":{}}],"execution_count":29},{"cell_type":"code","source":"train_data[train_data['clean_message'].apply(len) == maxi]","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":81},"id":"IpWtbxUzZM2o","outputId":"71b8e959-79e5-4d08-e981-d80b7b99483c","trusted":true,"execution":{"iopub.status.busy":"2025-01-08T06:26:53.057974Z","iopub.execute_input":"2025-01-08T06:26:53.058249Z","iopub.status.idle":"2025-01-08T06:26:53.076354Z","shell.execute_reply.started":"2025-01-08T06:26:53.058221Z","shell.execute_reply":"2025-01-08T06:26:53.075665Z"}},"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"                          id  anger  disgust  fear  joy  sadness lang  \\\n456  afr_train_track_a_00457      0        0     0    0        0  afr   \n\n                                         clean_message  \n456  die wysigingswetsontwerp op die provinsiale on...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>anger</th>\n      <th>disgust</th>\n      <th>fear</th>\n      <th>joy</th>\n      <th>sadness</th>\n      <th>lang</th>\n      <th>clean_message</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>456</th>\n      <td>afr_train_track_a_00457</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>afr</td>\n      <td>die wysigingswetsontwerp op die provinsiale on...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":30},{"cell_type":"code","source":"train_data[train_data['clean_message'].apply(len) == maxi][\"clean_message\"].values","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_gXtuVeaZREY","outputId":"69b121b6-04da-48a0-ac6e-4055e9dd4403","trusted":true,"execution":{"iopub.status.busy":"2025-01-08T06:26:53.077156Z","iopub.execute_input":"2025-01-08T06:26:53.077466Z","iopub.status.idle":"2025-01-08T06:26:53.091231Z","shell.execute_reply.started":"2025-01-08T06:26:53.077420Z","shell.execute_reply":"2025-01-08T06:26:53.090643Z"}},"outputs":[{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"array(['die wysigingswetsontwerp op die provinsiale ontwikkelingsraad en die wetsontwerp op die weskaapse jeugkommissie is deur die kabinet onderteken vir voorlegging aan die parlement'],\n      dtype=object)"},"metadata":{}}],"execution_count":31},{"cell_type":"code","source":"# shuffle data\ntrain_data=train_data.sample(frac=1)\n# train_data=train_data.sample(100,random_state=42)\n","metadata":{"execution":{"iopub.status.busy":"2025-01-08T06:26:53.091990Z","iopub.execute_input":"2025-01-08T06:26:53.092240Z","iopub.status.idle":"2025-01-08T06:26:53.106785Z","shell.execute_reply.started":"2025-01-08T06:26:53.092221Z","shell.execute_reply":"2025-01-08T06:26:53.105907Z"},"id":"Efx2E-A4Bad7","trusted":true},"outputs":[],"execution_count":32},{"cell_type":"code","source":"# train valid split\ntrain,valid=train_test_split(train_data,test_size=0.2,random_state=42)","metadata":{"execution":{"iopub.status.busy":"2025-01-08T06:26:53.107496Z","iopub.execute_input":"2025-01-08T06:26:53.107796Z","iopub.status.idle":"2025-01-08T06:26:53.122124Z","shell.execute_reply.started":"2025-01-08T06:26:53.107776Z","shell.execute_reply":"2025-01-08T06:26:53.121385Z"},"id":"QfMcAI26Bad7","trusted":true},"outputs":[],"execution_count":33},{"cell_type":"code","source":"x_train, y_train = train['clean_message'].values.tolist(), train[label_columns].values.tolist()\n\nx_valid, y_valid = valid['clean_message'].values.tolist(), valid[label_columns].values.tolist()","metadata":{"execution":{"iopub.status.busy":"2025-01-08T06:26:53.122924Z","iopub.execute_input":"2025-01-08T06:26:53.123208Z","iopub.status.idle":"2025-01-08T06:26:53.138272Z","shell.execute_reply.started":"2025-01-08T06:26:53.123179Z","shell.execute_reply":"2025-01-08T06:26:53.137504Z"},"id":"K0MsuYqpBad8","trusted":true},"outputs":[],"execution_count":34},{"cell_type":"code","source":"y_train_df = pd.DataFrame(y_train, columns=label_columns)","metadata":{"execution":{"iopub.status.busy":"2025-01-08T06:26:53.139080Z","iopub.execute_input":"2025-01-08T06:26:53.139333Z","iopub.status.idle":"2025-01-08T06:26:53.153788Z","shell.execute_reply.started":"2025-01-08T06:26:53.139313Z","shell.execute_reply":"2025-01-08T06:26:53.153103Z"},"id":"6Sgs63uPBad8","trusted":true},"outputs":[],"execution_count":35},{"cell_type":"code","source":"#update label column name\n\nlabel_distribution = y_train_df.apply(pd.Series.value_counts).T.fillna(0).astype(int)\n\nlabel_distribution.columns = ['count_0', 'count_1']\nlabel_distribution['sum'] = label_distribution['count_0'] + label_distribution['count_1']\nprint(label_distribution)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2025-01-08T06:26:53.154574Z","iopub.execute_input":"2025-01-08T06:26:53.154790Z","iopub.status.idle":"2025-01-08T06:26:53.174933Z","shell.execute_reply.started":"2025-01-08T06:26:53.154771Z","shell.execute_reply":"2025-01-08T06:26:53.174164Z"},"id":"sVD0DSPvmdcE","outputId":"6d9b07ad-be69-47b4-c19d-bb75b3e95810","trusted":true},"outputs":[{"name":"stdout","text":"         count_0  count_1  sum\nanger        941       36  977\ndisgust      968        9  977\nfear         879       98  977\njoy          550      427  977\nsadness      846      131  977\n","output_type":"stream"}],"execution_count":36},{"cell_type":"code","source":"device=\"cuda\" if torch.cuda.is_available() else 'cpu'","metadata":{"execution":{"iopub.status.busy":"2025-01-08T06:26:53.175718Z","iopub.execute_input":"2025-01-08T06:26:53.175896Z","iopub.status.idle":"2025-01-08T06:26:53.246887Z","shell.execute_reply.started":"2025-01-08T06:26:53.175880Z","shell.execute_reply":"2025-01-08T06:26:53.245927Z"},"id":"-EFMbbzALDd6","trusted":true},"outputs":[],"execution_count":37},{"cell_type":"code","source":"from sklearn.utils.class_weight import compute_class_weight\n\ncounts_0 = label_distribution['count_0'].to_numpy()\ncounts_1 = label_distribution['count_1'].to_numpy()\n\n# Compute class-specific weights for each label (each class)\nclass_weights = []\nfor i in range(len(counts_0)):\n    weight = compute_class_weight('balanced', classes=np.array([0, 1]), y=[0] * counts_0[i] + [1] * counts_1[i])\n    class_weights.append(weight)\n\n# Convert the list of weights to a tensor\nclass_weights_tensor = torch.tensor([w[1] for w in class_weights], dtype=torch.float,device=device)\nclass_weights_tensor","metadata":{"execution":{"iopub.status.busy":"2025-01-08T06:26:53.247731Z","iopub.execute_input":"2025-01-08T06:26:53.248034Z","iopub.status.idle":"2025-01-08T06:26:53.765692Z","shell.execute_reply.started":"2025-01-08T06:26:53.248004Z","shell.execute_reply":"2025-01-08T06:26:53.764958Z"},"trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"ggY-tV1n2P7Q","outputId":"bd44116c-7a53-4c44-c7b3-7b9e1f97133e"},"outputs":[{"execution_count":38,"output_type":"execute_result","data":{"text/plain":"tensor([13.5694, 54.2778,  4.9847,  1.1440,  3.7290], device='cuda:0')"},"metadata":{}}],"execution_count":38},{"cell_type":"code","source":"class Dataset(torch.utils.data.Dataset):\n\n    def __init__(self, encodings, labels):\n\n        self.encodings = encodings\n\n        self.labels = labels\n\n\n\n    def __getitem__(self, idx):\n\n        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n\n        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.float)\n\n        return item\n\n\n    def __len__(self):\n\n        return len(self.labels)","metadata":{"execution":{"iopub.status.busy":"2025-01-08T06:26:53.766453Z","iopub.execute_input":"2025-01-08T06:26:53.766726Z","iopub.status.idle":"2025-01-08T06:26:53.771643Z","shell.execute_reply.started":"2025-01-08T06:26:53.766704Z","shell.execute_reply":"2025-01-08T06:26:53.770761Z"},"id":"yZqp6Z2YmkJM","trusted":true},"outputs":[],"execution_count":39},{"cell_type":"code","source":"!ls output/best-model-v1","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2025-01-08T06:26:53.772240Z","iopub.execute_input":"2025-01-08T06:26:53.772472Z","iopub.status.idle":"2025-01-08T06:26:53.930213Z","shell.execute_reply.started":"2025-01-08T06:26:53.772452Z","shell.execute_reply":"2025-01-08T06:26:53.929133Z"},"id":"oKxH4x81Bad8","outputId":"4c211e4d-4b3a-4fa9-8868-978291f0419c","trusted":true},"outputs":[{"name":"stdout","text":"ls: cannot access 'output/best-model-v1': No such file or directory\n","output_type":"stream"},{"name":"stderr","text":"/usr/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"}],"execution_count":40},{"cell_type":"code","source":"def build_dataset(x_train,y_train,x_valid,y_valid,MAX_LENGTH):\n\n  train_encodings = tokenizer(x_train, truncation=True, padding=\"max_length\", max_length=MAX_LENGTH)\n\n  valid_encodings = tokenizer(x_valid, truncation=True, padding=\"max_length\", max_length=MAX_LENGTH)\n\n  train_dataset = Dataset(train_encodings, y_train)\n\n  valid_dataset = Dataset(valid_encodings, y_valid)\n\n  return train_dataset,valid_dataset\n\n\n\ndef train_arg():\n\n    training_args = TrainingArguments(\n\n    output_dir=\"./output/best-model-v1\",\n\n    report_to=None,\n    dataloader_pin_memory=False,\n\n    num_train_epochs=3,\n\n    gradient_accumulation_steps=2,\n\n    per_device_train_batch_size=8,\n\n    per_device_eval_batch_size=16,\n\n    learning_rate=2e-5,\n\n    weight_decay=0.01,\n\n    eval_strategy=IntervalStrategy.STEPS,\n\n    save_total_limit=1,\n\n    logging_dir=\"./logs\",\n\n    logging_strategy=IntervalStrategy.STEPS,\n\n    logging_steps=250,\n\n    eval_steps=500,\n\n    load_best_model_at_end=True,\n\n    lr_scheduler_type=\"cosine_with_restarts\",\n\n    warmup_steps=500,\n\n    save_strategy=\"steps\",\n\n    save_steps=500,\n\n  )\n\n    return training_args","metadata":{"execution":{"iopub.status.busy":"2025-01-08T06:26:53.931166Z","iopub.execute_input":"2025-01-08T06:26:53.931457Z","iopub.status.idle":"2025-01-08T06:26:53.937173Z","shell.execute_reply.started":"2025-01-08T06:26:53.931408Z","shell.execute_reply":"2025-01-08T06:26:53.936505Z"},"id":"rkp5NCQoml0I","trusted":true},"outputs":[],"execution_count":41},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, precision_recall_fscore_support\nimport numpy as np\n\ndef compute_metrics(eval_pred):\n    \"\"\"\n    Compute metrics for multi-label classification.\n\n    Args:\n        eval_pred: Tuple of (predictions, labels).\n            predictions: logits or probabilities of shape (batch_size, num_labels)\n            labels: ground truth of shape (batch_size, num_labels)\n\n    Returns:\n        metrics: Dictionary containing accuracy, precision, recall, and F1-score.\n    \"\"\"\n    # Unpack predictions and labels\n    logits, labels = eval_pred\n\n    # Convert logits to binary predictions\n    # If your model outputs probabilities (e.g., after sigmoid), use threshold=0.5\n    probabilities = expit(logits)\n    predictions = (probabilities > 0.5).astype(int)\n\n    # Compute accuracy\n    accuracy = accuracy_score(labels, predictions)\n\n    # Compute precision, recall, f1-score\n    precision, recall, f1, _ = precision_recall_fscore_support(\n        labels, predictions, average=None\n    )\n\n    return {\n        \"accuracy\": accuracy,\n        \"precision\": precision,\n        \"recall\": recall,\n        \"f1\": f1\n    }","metadata":{"execution":{"iopub.status.busy":"2025-01-08T06:26:53.938049Z","iopub.execute_input":"2025-01-08T06:26:53.938333Z","iopub.status.idle":"2025-01-08T06:26:53.956812Z","shell.execute_reply.started":"2025-01-08T06:26:53.938303Z","shell.execute_reply":"2025-01-08T06:26:53.956133Z"},"id":"izahhMy5NwVv","trusted":true},"outputs":[],"execution_count":42},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\nfrom scipy.special import expit  # For sigmoid activation if needed\n\ndef evaluate_model(model, valid_dataset, compute_metrics, batch_size=32, device=\"cuda\",average=None):\n    \"\"\"\n    Evaluate a multi-label classification model without using the Hugging Face Trainer.\n\n    Args:\n        model: The trained model (Hugging Face model).\n        valid_dataset: The validation dataset (torch.utils.data.Dataset).\n        compute_metrics: The function to compute evaluation metrics.\n        batch_size: Batch size for evaluation.\n        device: Device to perform evaluation (e.g., \"cuda\" or \"cpu\").\n\n    Returns:\n        metrics: Dictionary containing evaluation metrics.\n    \"\"\"\n    # Set model to evaluation mode\n    model.eval()\n    model.to(device)\n\n    # Create DataLoader for the validation dataset\n    valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n\n    all_predictions = []\n    all_labels = []\n\n    # Disable gradient computation for evaluation\n    with torch.no_grad():\n        for batch in tqdm(valid_loader, desc=\"Evaluating\"):\n            # Get inputs and labels\n            inputs = {key: val.to(device) for key, val in batch.items() if key != \"labels\"}\n            labels = batch[\"labels\"].to(device)\n\n            # Forward pass: Get logits\n            logits = model(**inputs)\n\n            # Apply sigmoid activation to logits if necessary (for multi-label classification)\n            probabilities = expit(logits.cpu().numpy())  # Convert logits to probabilities\n            predictions = (probabilities > 0.5).astype(int)  # Convert probabilities to binary predictions\n\n            # Collect predictions and labels\n            all_predictions.append(predictions)\n            all_labels.append(labels.cpu().numpy())\n    print()\n    # Concatenate all predictions and labels\n    all_predictions = np.concatenate(all_predictions, axis=0)\n    all_labels = np.concatenate(all_labels, axis=0)\n\n    # Unpack predictions and labels\n    logits, labels = all_predictions,all_labels\n\n    # Convert logits to binary predictions\n    # If your model outputs probabilities (e.g., after sigmoid), use threshold=0.5\n    probabilities = expit(logits)\n    predictions = (probabilities > 0.5).astype(int)\n\n    # Compute accuracy\n    accuracy = accuracy_score(labels, predictions)\n\n    # Compute precision, recall, f1-score\n    precision, recall, f1, _ = precision_recall_fscore_support(\n        labels, predictions, average=average\n    )\n    return {\n        \"accuracy\": accuracy,\n        \"precision\": precision,\n        \"recall\": recall,\n        \"f1\": f1\n    },all_predictions","metadata":{"id":"cnH_BQ0XQeVZ","trusted":true,"execution":{"iopub.status.busy":"2025-01-08T06:26:53.957530Z","iopub.execute_input":"2025-01-08T06:26:53.957775Z","iopub.status.idle":"2025-01-08T06:26:53.970020Z","shell.execute_reply.started":"2025-01-08T06:26:53.957755Z","shell.execute_reply":"2025-01-08T06:26:53.969294Z"}},"outputs":[],"execution_count":43},{"cell_type":"code","source":"def evaluate_func(trainer):\n    metrics = evaluate_model(\n    model=model,\n    valid_dataset=valid_dataset,\n    compute_metrics=compute_metrics,\n    batch_size=32,\n    device=device\n)\n    accuracy=metrics[\"accuracy\"]\n    precision=metrics[\"precision\"]\n    recall=metrics[\"recall\"]\n    f1=metrics[\"f1\"]\n\n    return precision, recall, f1, accuracy","metadata":{"execution":{"iopub.status.busy":"2025-01-08T06:26:53.970873Z","iopub.execute_input":"2025-01-08T06:26:53.971156Z","iopub.status.idle":"2025-01-08T06:26:53.986684Z","shell.execute_reply.started":"2025-01-08T06:26:53.971126Z","shell.execute_reply":"2025-01-08T06:26:53.985759Z"},"id":"ahrCmCKmmvvV","trusted":true},"outputs":[],"execution_count":44},{"cell_type":"code","source":"MAX_LENGTH = 512\n\nmetrics_df = pd.DataFrame(columns=[\"Models_name\", \"Precision\", \"Recall\", \"F1_score\", \"Accuracy\"])\n\nconfusion_matrices = []","metadata":{"execution":{"iopub.status.busy":"2025-01-08T06:26:53.987662Z","iopub.execute_input":"2025-01-08T06:26:53.987946Z","iopub.status.idle":"2025-01-08T06:26:54.007160Z","shell.execute_reply.started":"2025-01-08T06:26:53.987915Z","shell.execute_reply":"2025-01-08T06:26:54.006302Z"},"id":"_2K_WqEPnF6l","trusted":true},"outputs":[],"execution_count":45},{"cell_type":"code","source":"quantization_config = BitsAndBytesConfig(\n\n    load_in_4bit=True,\n\n    bnb_4bit_use_double_quant=True,\n\n    bnb_4bit_quant_type=\"nf4\",\n\n    bnb_4bit_compute_dtype=torch.bfloat16,\n\n)","metadata":{"execution":{"iopub.status.busy":"2025-01-08T06:26:54.007936Z","iopub.execute_input":"2025-01-08T06:26:54.008214Z","iopub.status.idle":"2025-01-08T06:26:54.022695Z","shell.execute_reply.started":"2025-01-08T06:26:54.008185Z","shell.execute_reply":"2025-01-08T06:26:54.022034Z"},"id":"3TcPpfq1GdUI","trusted":true},"outputs":[],"execution_count":46},{"cell_type":"code","source":"models_list=models_list_3\n\nmodels_name=models_name_3\n\nx=models_list[0]","metadata":{"execution":{"iopub.status.busy":"2025-01-08T06:26:54.023347Z","iopub.execute_input":"2025-01-08T06:26:54.023581Z","iopub.status.idle":"2025-01-08T06:26:54.036596Z","shell.execute_reply.started":"2025-01-08T06:26:54.023561Z","shell.execute_reply":"2025-01-08T06:26:54.035932Z"},"id":"ps6BcVspzvnP","trusted":true},"outputs":[],"execution_count":47},{"cell_type":"code","source":"token=\"\"\n\nimport huggingface_hub\n\nif \"gemma\" in x or \"Phi\" in x:\n\n  huggingface_hub.login(token=token)","metadata":{"execution":{"iopub.status.busy":"2025-01-08T06:26:54.037319Z","iopub.execute_input":"2025-01-08T06:26:54.037528Z","iopub.status.idle":"2025-01-08T06:26:54.050948Z","shell.execute_reply.started":"2025-01-08T06:26:54.037510Z","shell.execute_reply":"2025-01-08T06:26:54.050139Z"},"id":"jHjrZwu7_GSr","trusted":true},"outputs":[],"execution_count":48},{"cell_type":"code","source":"USE_LORA=False","metadata":{"execution":{"iopub.status.busy":"2025-01-08T06:26:54.051780Z","iopub.execute_input":"2025-01-08T06:26:54.052059Z","iopub.status.idle":"2025-01-08T06:26:54.064854Z","shell.execute_reply.started":"2025-01-08T06:26:54.052030Z","shell.execute_reply":"2025-01-08T06:26:54.064127Z"},"id":"cclf1eMoIfuA","trusted":true},"outputs":[],"execution_count":49},{"cell_type":"code","source":"!pip install -q sentence_transformers","metadata":{"execution":{"iopub.status.busy":"2025-01-08T06:26:54.065616Z","iopub.execute_input":"2025-01-08T06:26:54.065892Z","iopub.status.idle":"2025-01-08T06:26:57.811992Z","shell.execute_reply.started":"2025-01-08T06:26:54.065864Z","shell.execute_reply":"2025-01-08T06:26:57.810983Z"},"id":"cclf1eMoIfuA","trusted":true},"outputs":[{"name":"stderr","text":"/usr/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":50},{"cell_type":"code","source":"from transformers import AutoModel\nfrom sentence_transformers import SentenceTransformer","metadata":{"id":"4uSE7TEYBGfN","trusted":true,"execution":{"iopub.status.busy":"2025-01-08T06:26:57.813005Z","iopub.execute_input":"2025-01-08T06:26:57.813242Z","iopub.status.idle":"2025-01-08T06:26:57.869545Z","shell.execute_reply.started":"2025-01-08T06:26:57.813224Z","shell.execute_reply":"2025-01-08T06:26:57.868955Z"}},"outputs":[],"execution_count":51},{"cell_type":"code","source":"\n\ntorch.cuda.empty_cache()\n\n\n\nprint(x)\nuse_gpu = torch.cuda.is_available()\nembedding_model = SentenceTransformer(\n        x,\n        trust_remote_code=True,\n        device=\"cpu\" if not use_gpu else \"cuda\",\n        model_kwargs={\n            \"torch_dtype\": torch.bfloat16 if use_gpu else torch.float32,\n            \"attn_implementation\": \"sdpa\"\n        },\n        # vector_dim must be 12288, 1024, 512, 256\n        ## 1024 is recommended\n        # set is_text_encoder 'True', if you do not encode image\n        config_kwargs={\"is_text_encoder\": True, \"vector_dim\": 1024},\n    )\n    # We can reduce the max_seq_length from the default of 2048 for faster encoding\nembedding_model.max_seq_length = MAX_LENGTH","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2025-01-08T06:26:57.870223Z","iopub.execute_input":"2025-01-08T06:26:57.870486Z","iopub.status.idle":"2025-01-08T06:28:47.729256Z","shell.execute_reply.started":"2025-01-08T06:26:57.870455Z","shell.execute_reply":"2025-01-08T06:28:47.728604Z"},"id":"RTwQtNnRZXNI","outputId":"ed7ecfbf-7c7c-4e94-a7b3-131849e04b64","trusted":true},"outputs":[{"name":"stdout","text":"infgrad/jasper_en_vision_language_v1\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/224 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1609a82b8b5545cbb0ec6d90addb44dd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/397 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6142af09c16340e6aa72bcd541906cfc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/246k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5af074fa01ba42dd85039ac9490ce69a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"custom_st.py:   0%|          | 0.00/3.12k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"56b7385be308413c98871a9d1a6695c3"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/infgrad/jasper_en_vision_language_v1:\n- custom_st.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/105 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cbb18c3c42664a5cb8e3b055701225bf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.26k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"edc7e34f80244407a70982b1644f48ea"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"configuration_jasper_vl.py:   0%|          | 0.00/2.39k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"99690e2cb58d43ffa20337c8aa982c05"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/infgrad/jasper_en_vision_language_v1:\n- configuration_jasper_vl.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modeling_jasper_vl.py:   0%|          | 0.00/55.3k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b312e0c8c79249a792cbd3e0ea3ef0fe"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/infgrad/jasper_en_vision_language_v1:\n- modeling_jasper_vl.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/3.99G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4dbf0c0d037541e19a1175b9f1313d1e"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at infgrad/jasper_en_vision_language_v1 were not used when initializing JasperVL: ['vision_model.vision_model.embeddings.patch_embedding.bias', 'vision_model.vision_model.embeddings.patch_embedding.weight', 'vision_model.vision_model.embeddings.position_embedding.weight', 'vision_model.vision_model.encoder.layers.0.layer_norm1.bias', 'vision_model.vision_model.encoder.layers.0.layer_norm1.weight', 'vision_model.vision_model.encoder.layers.0.layer_norm2.bias', 'vision_model.vision_model.encoder.layers.0.layer_norm2.weight', 'vision_model.vision_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.vision_model.encoder.layers.0.mlp.fc2.bias', 'vision_model.vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'vision_model.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'vision_model.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'vision_model.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'vision_model.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'vision_model.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'vision_model.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'vision_model.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'vision_model.vision_model.encoder.layers.1.layer_norm1.bias', 'vision_model.vision_model.encoder.layers.1.layer_norm1.weight', 'vision_model.vision_model.encoder.layers.1.layer_norm2.bias', 'vision_model.vision_model.encoder.layers.1.layer_norm2.weight', 'vision_model.vision_model.encoder.layers.1.mlp.fc1.bias', 'vision_model.vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.vision_model.encoder.layers.1.mlp.fc2.bias', 'vision_model.vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'vision_model.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'vision_model.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'vision_model.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'vision_model.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'vision_model.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'vision_model.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'vision_model.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'vision_model.vision_model.encoder.layers.10.layer_norm1.bias', 'vision_model.vision_model.encoder.layers.10.layer_norm1.weight', 'vision_model.vision_model.encoder.layers.10.layer_norm2.bias', 'vision_model.vision_model.encoder.layers.10.layer_norm2.weight', 'vision_model.vision_model.encoder.layers.10.mlp.fc1.bias', 'vision_model.vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.vision_model.encoder.layers.10.mlp.fc2.bias', 'vision_model.vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'vision_model.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'vision_model.vision_model.encoder.layers.10.self_attn.out_proj.bias', 'vision_model.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'vision_model.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'vision_model.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'vision_model.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'vision_model.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'vision_model.vision_model.encoder.layers.11.layer_norm1.bias', 'vision_model.vision_model.encoder.layers.11.layer_norm1.weight', 'vision_model.vision_model.encoder.layers.11.layer_norm2.bias', 'vision_model.vision_model.encoder.layers.11.layer_norm2.weight', 'vision_model.vision_model.encoder.layers.11.mlp.fc1.bias', 'vision_model.vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.vision_model.encoder.layers.11.mlp.fc2.bias', 'vision_model.vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'vision_model.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'vision_model.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'vision_model.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'vision_model.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'vision_model.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'vision_model.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'vision_model.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'vision_model.vision_model.encoder.layers.12.layer_norm1.bias', 'vision_model.vision_model.encoder.layers.12.layer_norm1.weight', 'vision_model.vision_model.encoder.layers.12.layer_norm2.bias', 'vision_model.vision_model.encoder.layers.12.layer_norm2.weight', 'vision_model.vision_model.encoder.layers.12.mlp.fc1.bias', 'vision_model.vision_model.encoder.layers.12.mlp.fc1.weight', 'vision_model.vision_model.encoder.layers.12.mlp.fc2.bias', 'vision_model.vision_model.encoder.layers.12.mlp.fc2.weight', 'vision_model.vision_model.encoder.layers.12.self_attn.k_proj.bias', 'vision_model.vision_model.encoder.layers.12.self_attn.k_proj.weight', 'vision_model.vision_model.encoder.layers.12.self_attn.out_proj.bias', 'vision_model.vision_model.encoder.layers.12.self_attn.out_proj.weight', 'vision_model.vision_model.encoder.layers.12.self_attn.q_proj.bias', 'vision_model.vision_model.encoder.layers.12.self_attn.q_proj.weight', 'vision_model.vision_model.encoder.layers.12.self_attn.v_proj.bias', 'vision_model.vision_model.encoder.layers.12.self_attn.v_proj.weight', 'vision_model.vision_model.encoder.layers.13.layer_norm1.bias', 'vision_model.vision_model.encoder.layers.13.layer_norm1.weight', 'vision_model.vision_model.encoder.layers.13.layer_norm2.bias', 'vision_model.vision_model.encoder.layers.13.layer_norm2.weight', 'vision_model.vision_model.encoder.layers.13.mlp.fc1.bias', 'vision_model.vision_model.encoder.layers.13.mlp.fc1.weight', 'vision_model.vision_model.encoder.layers.13.mlp.fc2.bias', 'vision_model.vision_model.encoder.layers.13.mlp.fc2.weight', 'vision_model.vision_model.encoder.layers.13.self_attn.k_proj.bias', 'vision_model.vision_model.encoder.layers.13.self_attn.k_proj.weight', 'vision_model.vision_model.encoder.layers.13.self_attn.out_proj.bias', 'vision_model.vision_model.encoder.layers.13.self_attn.out_proj.weight', 'vision_model.vision_model.encoder.layers.13.self_attn.q_proj.bias', 'vision_model.vision_model.encoder.layers.13.self_attn.q_proj.weight', 'vision_model.vision_model.encoder.layers.13.self_attn.v_proj.bias', 'vision_model.vision_model.encoder.layers.13.self_attn.v_proj.weight', 'vision_model.vision_model.encoder.layers.14.layer_norm1.bias', 'vision_model.vision_model.encoder.layers.14.layer_norm1.weight', 'vision_model.vision_model.encoder.layers.14.layer_norm2.bias', 'vision_model.vision_model.encoder.layers.14.layer_norm2.weight', 'vision_model.vision_model.encoder.layers.14.mlp.fc1.bias', 'vision_model.vision_model.encoder.layers.14.mlp.fc1.weight', 'vision_model.vision_model.encoder.layers.14.mlp.fc2.bias', 'vision_model.vision_model.encoder.layers.14.mlp.fc2.weight', 'vision_model.vision_model.encoder.layers.14.self_attn.k_proj.bias', 'vision_model.vision_model.encoder.layers.14.self_attn.k_proj.weight', 'vision_model.vision_model.encoder.layers.14.self_attn.out_proj.bias', 'vision_model.vision_model.encoder.layers.14.self_attn.out_proj.weight', 'vision_model.vision_model.encoder.layers.14.self_attn.q_proj.bias', 'vision_model.vision_model.encoder.layers.14.self_attn.q_proj.weight', 'vision_model.vision_model.encoder.layers.14.self_attn.v_proj.bias', 'vision_model.vision_model.encoder.layers.14.self_attn.v_proj.weight', 'vision_model.vision_model.encoder.layers.15.layer_norm1.bias', 'vision_model.vision_model.encoder.layers.15.layer_norm1.weight', 'vision_model.vision_model.encoder.layers.15.layer_norm2.bias', 'vision_model.vision_model.encoder.layers.15.layer_norm2.weight', 'vision_model.vision_model.encoder.layers.15.mlp.fc1.bias', 'vision_model.vision_model.encoder.layers.15.mlp.fc1.weight', 'vision_model.vision_model.encoder.layers.15.mlp.fc2.bias', 'vision_model.vision_model.encoder.layers.15.mlp.fc2.weight', 'vision_model.vision_model.encoder.layers.15.self_attn.k_proj.bias', 'vision_model.vision_model.encoder.layers.15.self_attn.k_proj.weight', 'vision_model.vision_model.encoder.layers.15.self_attn.out_proj.bias', 'vision_model.vision_model.encoder.layers.15.self_attn.out_proj.weight', 'vision_model.vision_model.encoder.layers.15.self_attn.q_proj.bias', 'vision_model.vision_model.encoder.layers.15.self_attn.q_proj.weight', 'vision_model.vision_model.encoder.layers.15.self_attn.v_proj.bias', 'vision_model.vision_model.encoder.layers.15.self_attn.v_proj.weight', 'vision_model.vision_model.encoder.layers.16.layer_norm1.bias', 'vision_model.vision_model.encoder.layers.16.layer_norm1.weight', 'vision_model.vision_model.encoder.layers.16.layer_norm2.bias', 'vision_model.vision_model.encoder.layers.16.layer_norm2.weight', 'vision_model.vision_model.encoder.layers.16.mlp.fc1.bias', 'vision_model.vision_model.encoder.layers.16.mlp.fc1.weight', 'vision_model.vision_model.encoder.layers.16.mlp.fc2.bias', 'vision_model.vision_model.encoder.layers.16.mlp.fc2.weight', 'vision_model.vision_model.encoder.layers.16.self_attn.k_proj.bias', 'vision_model.vision_model.encoder.layers.16.self_attn.k_proj.weight', 'vision_model.vision_model.encoder.layers.16.self_attn.out_proj.bias', 'vision_model.vision_model.encoder.layers.16.self_attn.out_proj.weight', 'vision_model.vision_model.encoder.layers.16.self_attn.q_proj.bias', 'vision_model.vision_model.encoder.layers.16.self_attn.q_proj.weight', 'vision_model.vision_model.encoder.layers.16.self_attn.v_proj.bias', 'vision_model.vision_model.encoder.layers.16.self_attn.v_proj.weight', 'vision_model.vision_model.encoder.layers.17.layer_norm1.bias', 'vision_model.vision_model.encoder.layers.17.layer_norm1.weight', 'vision_model.vision_model.encoder.layers.17.layer_norm2.bias', 'vision_model.vision_model.encoder.layers.17.layer_norm2.weight', 'vision_model.vision_model.encoder.layers.17.mlp.fc1.bias', 'vision_model.vision_model.encoder.layers.17.mlp.fc1.weight', 'vision_model.vision_model.encoder.layers.17.mlp.fc2.bias', 'vision_model.vision_model.encoder.layers.17.mlp.fc2.weight', 'vision_model.vision_model.encoder.layers.17.self_attn.k_proj.bias', 'vision_model.vision_model.encoder.layers.17.self_attn.k_proj.weight', 'vision_model.vision_model.encoder.layers.17.self_attn.out_proj.bias', 'vision_model.vision_model.encoder.layers.17.self_attn.out_proj.weight', 'vision_model.vision_model.encoder.layers.17.self_attn.q_proj.bias', 'vision_model.vision_model.encoder.layers.17.self_attn.q_proj.weight', 'vision_model.vision_model.encoder.layers.17.self_attn.v_proj.bias', 'vision_model.vision_model.encoder.layers.17.self_attn.v_proj.weight', 'vision_model.vision_model.encoder.layers.18.layer_norm1.bias', 'vision_model.vision_model.encoder.layers.18.layer_norm1.weight', 'vision_model.vision_model.encoder.layers.18.layer_norm2.bias', 'vision_model.vision_model.encoder.layers.18.layer_norm2.weight', 'vision_model.vision_model.encoder.layers.18.mlp.fc1.bias', 'vision_model.vision_model.encoder.layers.18.mlp.fc1.weight', 'vision_model.vision_model.encoder.layers.18.mlp.fc2.bias', 'vision_model.vision_model.encoder.layers.18.mlp.fc2.weight', 'vision_model.vision_model.encoder.layers.18.self_attn.k_proj.bias', 'vision_model.vision_model.encoder.layers.18.self_attn.k_proj.weight', 'vision_model.vision_model.encoder.layers.18.self_attn.out_proj.bias', 'vision_model.vision_model.encoder.layers.18.self_attn.out_proj.weight', 'vision_model.vision_model.encoder.layers.18.self_attn.q_proj.bias', 'vision_model.vision_model.encoder.layers.18.self_attn.q_proj.weight', 'vision_model.vision_model.encoder.layers.18.self_attn.v_proj.bias', 'vision_model.vision_model.encoder.layers.18.self_attn.v_proj.weight', 'vision_model.vision_model.encoder.layers.19.layer_norm1.bias', 'vision_model.vision_model.encoder.layers.19.layer_norm1.weight', 'vision_model.vision_model.encoder.layers.19.layer_norm2.bias', 'vision_model.vision_model.encoder.layers.19.layer_norm2.weight', 'vision_model.vision_model.encoder.layers.19.mlp.fc1.bias', 'vision_model.vision_model.encoder.layers.19.mlp.fc1.weight', 'vision_model.vision_model.encoder.layers.19.mlp.fc2.bias', 'vision_model.vision_model.encoder.layers.19.mlp.fc2.weight', 'vision_model.vision_model.encoder.layers.19.self_attn.k_proj.bias', 'vision_model.vision_model.encoder.layers.19.self_attn.k_proj.weight', 'vision_model.vision_model.encoder.layers.19.self_attn.out_proj.bias', 'vision_model.vision_model.encoder.layers.19.self_attn.out_proj.weight', 'vision_model.vision_model.encoder.layers.19.self_attn.q_proj.bias', 'vision_model.vision_model.encoder.layers.19.self_attn.q_proj.weight', 'vision_model.vision_model.encoder.layers.19.self_attn.v_proj.bias', 'vision_model.vision_model.encoder.layers.19.self_attn.v_proj.weight', 'vision_model.vision_model.encoder.layers.2.layer_norm1.bias', 'vision_model.vision_model.encoder.layers.2.layer_norm1.weight', 'vision_model.vision_model.encoder.layers.2.layer_norm2.bias', 'vision_model.vision_model.encoder.layers.2.layer_norm2.weight', 'vision_model.vision_model.encoder.layers.2.mlp.fc1.bias', 'vision_model.vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'vision_model.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'vision_model.vision_model.encoder.layers.2.self_attn.out_proj.bias', 'vision_model.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'vision_model.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'vision_model.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'vision_model.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'vision_model.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'vision_model.vision_model.encoder.layers.20.layer_norm1.bias', 'vision_model.vision_model.encoder.layers.20.layer_norm1.weight', 'vision_model.vision_model.encoder.layers.20.layer_norm2.bias', 'vision_model.vision_model.encoder.layers.20.layer_norm2.weight', 'vision_model.vision_model.encoder.layers.20.mlp.fc1.bias', 'vision_model.vision_model.encoder.layers.20.mlp.fc1.weight', 'vision_model.vision_model.encoder.layers.20.mlp.fc2.bias', 'vision_model.vision_model.encoder.layers.20.mlp.fc2.weight', 'vision_model.vision_model.encoder.layers.20.self_attn.k_proj.bias', 'vision_model.vision_model.encoder.layers.20.self_attn.k_proj.weight', 'vision_model.vision_model.encoder.layers.20.self_attn.out_proj.bias', 'vision_model.vision_model.encoder.layers.20.self_attn.out_proj.weight', 'vision_model.vision_model.encoder.layers.20.self_attn.q_proj.bias', 'vision_model.vision_model.encoder.layers.20.self_attn.q_proj.weight', 'vision_model.vision_model.encoder.layers.20.self_attn.v_proj.bias', 'vision_model.vision_model.encoder.layers.20.self_attn.v_proj.weight', 'vision_model.vision_model.encoder.layers.21.layer_norm1.bias', 'vision_model.vision_model.encoder.layers.21.layer_norm1.weight', 'vision_model.vision_model.encoder.layers.21.layer_norm2.bias', 'vision_model.vision_model.encoder.layers.21.layer_norm2.weight', 'vision_model.vision_model.encoder.layers.21.mlp.fc1.bias', 'vision_model.vision_model.encoder.layers.21.mlp.fc1.weight', 'vision_model.vision_model.encoder.layers.21.mlp.fc2.bias', 'vision_model.vision_model.encoder.layers.21.mlp.fc2.weight', 'vision_model.vision_model.encoder.layers.21.self_attn.k_proj.bias', 'vision_model.vision_model.encoder.layers.21.self_attn.k_proj.weight', 'vision_model.vision_model.encoder.layers.21.self_attn.out_proj.bias', 'vision_model.vision_model.encoder.layers.21.self_attn.out_proj.weight', 'vision_model.vision_model.encoder.layers.21.self_attn.q_proj.bias', 'vision_model.vision_model.encoder.layers.21.self_attn.q_proj.weight', 'vision_model.vision_model.encoder.layers.21.self_attn.v_proj.bias', 'vision_model.vision_model.encoder.layers.21.self_attn.v_proj.weight', 'vision_model.vision_model.encoder.layers.22.layer_norm1.bias', 'vision_model.vision_model.encoder.layers.22.layer_norm1.weight', 'vision_model.vision_model.encoder.layers.22.layer_norm2.bias', 'vision_model.vision_model.encoder.layers.22.layer_norm2.weight', 'vision_model.vision_model.encoder.layers.22.mlp.fc1.bias', 'vision_model.vision_model.encoder.layers.22.mlp.fc1.weight', 'vision_model.vision_model.encoder.layers.22.mlp.fc2.bias', 'vision_model.vision_model.encoder.layers.22.mlp.fc2.weight', 'vision_model.vision_model.encoder.layers.22.self_attn.k_proj.bias', 'vision_model.vision_model.encoder.layers.22.self_attn.k_proj.weight', 'vision_model.vision_model.encoder.layers.22.self_attn.out_proj.bias', 'vision_model.vision_model.encoder.layers.22.self_attn.out_proj.weight', 'vision_model.vision_model.encoder.layers.22.self_attn.q_proj.bias', 'vision_model.vision_model.encoder.layers.22.self_attn.q_proj.weight', 'vision_model.vision_model.encoder.layers.22.self_attn.v_proj.bias', 'vision_model.vision_model.encoder.layers.22.self_attn.v_proj.weight', 'vision_model.vision_model.encoder.layers.23.layer_norm1.bias', 'vision_model.vision_model.encoder.layers.23.layer_norm1.weight', 'vision_model.vision_model.encoder.layers.23.layer_norm2.bias', 'vision_model.vision_model.encoder.layers.23.layer_norm2.weight', 'vision_model.vision_model.encoder.layers.23.mlp.fc1.bias', 'vision_model.vision_model.encoder.layers.23.mlp.fc1.weight', 'vision_model.vision_model.encoder.layers.23.mlp.fc2.bias', 'vision_model.vision_model.encoder.layers.23.mlp.fc2.weight', 'vision_model.vision_model.encoder.layers.23.self_attn.k_proj.bias', 'vision_model.vision_model.encoder.layers.23.self_attn.k_proj.weight', 'vision_model.vision_model.encoder.layers.23.self_attn.out_proj.bias', 'vision_model.vision_model.encoder.layers.23.self_attn.out_proj.weight', 'vision_model.vision_model.encoder.layers.23.self_attn.q_proj.bias', 'vision_model.vision_model.encoder.layers.23.self_attn.q_proj.weight', 'vision_model.vision_model.encoder.layers.23.self_attn.v_proj.bias', 'vision_model.vision_model.encoder.layers.23.self_attn.v_proj.weight', 'vision_model.vision_model.encoder.layers.24.layer_norm1.bias', 'vision_model.vision_model.encoder.layers.24.layer_norm1.weight', 'vision_model.vision_model.encoder.layers.24.layer_norm2.bias', 'vision_model.vision_model.encoder.layers.24.layer_norm2.weight', 'vision_model.vision_model.encoder.layers.24.mlp.fc1.bias', 'vision_model.vision_model.encoder.layers.24.mlp.fc1.weight', 'vision_model.vision_model.encoder.layers.24.mlp.fc2.bias', 'vision_model.vision_model.encoder.layers.24.mlp.fc2.weight', 'vision_model.vision_model.encoder.layers.24.self_attn.k_proj.bias', 'vision_model.vision_model.encoder.layers.24.self_attn.k_proj.weight', 'vision_model.vision_model.encoder.layers.24.self_attn.out_proj.bias', 'vision_model.vision_model.encoder.layers.24.self_attn.out_proj.weight', 'vision_model.vision_model.encoder.layers.24.self_attn.q_proj.bias', 'vision_model.vision_model.encoder.layers.24.self_attn.q_proj.weight', 'vision_model.vision_model.encoder.layers.24.self_attn.v_proj.bias', 'vision_model.vision_model.encoder.layers.24.self_attn.v_proj.weight', 'vision_model.vision_model.encoder.layers.25.layer_norm1.bias', 'vision_model.vision_model.encoder.layers.25.layer_norm1.weight', 'vision_model.vision_model.encoder.layers.25.layer_norm2.bias', 'vision_model.vision_model.encoder.layers.25.layer_norm2.weight', 'vision_model.vision_model.encoder.layers.25.mlp.fc1.bias', 'vision_model.vision_model.encoder.layers.25.mlp.fc1.weight', 'vision_model.vision_model.encoder.layers.25.mlp.fc2.bias', 'vision_model.vision_model.encoder.layers.25.mlp.fc2.weight', 'vision_model.vision_model.encoder.layers.25.self_attn.k_proj.bias', 'vision_model.vision_model.encoder.layers.25.self_attn.k_proj.weight', 'vision_model.vision_model.encoder.layers.25.self_attn.out_proj.bias', 'vision_model.vision_model.encoder.layers.25.self_attn.out_proj.weight', 'vision_model.vision_model.encoder.layers.25.self_attn.q_proj.bias', 'vision_model.vision_model.encoder.layers.25.self_attn.q_proj.weight', 'vision_model.vision_model.encoder.layers.25.self_attn.v_proj.bias', 'vision_model.vision_model.encoder.layers.25.self_attn.v_proj.weight', 'vision_model.vision_model.encoder.layers.26.layer_norm1.bias', 'vision_model.vision_model.encoder.layers.26.layer_norm1.weight', 'vision_model.vision_model.encoder.layers.26.layer_norm2.bias', 'vision_model.vision_model.encoder.layers.26.layer_norm2.weight', 'vision_model.vision_model.encoder.layers.26.mlp.fc1.bias', 'vision_model.vision_model.encoder.layers.26.mlp.fc1.weight', 'vision_model.vision_model.encoder.layers.26.mlp.fc2.bias', 'vision_model.vision_model.encoder.layers.26.mlp.fc2.weight', 'vision_model.vision_model.encoder.layers.26.self_attn.k_proj.bias', 'vision_model.vision_model.encoder.layers.26.self_attn.k_proj.weight', 'vision_model.vision_model.encoder.layers.26.self_attn.out_proj.bias', 'vision_model.vision_model.encoder.layers.26.self_attn.out_proj.weight', 'vision_model.vision_model.encoder.layers.26.self_attn.q_proj.bias', 'vision_model.vision_model.encoder.layers.26.self_attn.q_proj.weight', 'vision_model.vision_model.encoder.layers.26.self_attn.v_proj.bias', 'vision_model.vision_model.encoder.layers.26.self_attn.v_proj.weight', 'vision_model.vision_model.encoder.layers.3.layer_norm1.bias', 'vision_model.vision_model.encoder.layers.3.layer_norm1.weight', 'vision_model.vision_model.encoder.layers.3.layer_norm2.bias', 'vision_model.vision_model.encoder.layers.3.layer_norm2.weight', 'vision_model.vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.vision_model.encoder.layers.3.mlp.fc2.bias', 'vision_model.vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'vision_model.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'vision_model.vision_model.encoder.layers.3.self_attn.out_proj.bias', 'vision_model.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'vision_model.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'vision_model.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'vision_model.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'vision_model.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'vision_model.vision_model.encoder.layers.4.layer_norm1.bias', 'vision_model.vision_model.encoder.layers.4.layer_norm1.weight', 'vision_model.vision_model.encoder.layers.4.layer_norm2.bias', 'vision_model.vision_model.encoder.layers.4.layer_norm2.weight', 'vision_model.vision_model.encoder.layers.4.mlp.fc1.bias', 'vision_model.vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.vision_model.encoder.layers.4.mlp.fc2.bias', 'vision_model.vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'vision_model.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'vision_model.vision_model.encoder.layers.4.self_attn.out_proj.bias', 'vision_model.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'vision_model.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'vision_model.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'vision_model.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'vision_model.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'vision_model.vision_model.encoder.layers.5.layer_norm1.bias', 'vision_model.vision_model.encoder.layers.5.layer_norm1.weight', 'vision_model.vision_model.encoder.layers.5.layer_norm2.bias', 'vision_model.vision_model.encoder.layers.5.layer_norm2.weight', 'vision_model.vision_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'vision_model.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'vision_model.vision_model.encoder.layers.5.self_attn.out_proj.bias', 'vision_model.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'vision_model.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'vision_model.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'vision_model.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'vision_model.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'vision_model.vision_model.encoder.layers.6.layer_norm1.bias', 'vision_model.vision_model.encoder.layers.6.layer_norm1.weight', 'vision_model.vision_model.encoder.layers.6.layer_norm2.bias', 'vision_model.vision_model.encoder.layers.6.layer_norm2.weight', 'vision_model.vision_model.encoder.layers.6.mlp.fc1.bias', 'vision_model.vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.vision_model.encoder.layers.6.mlp.fc2.bias', 'vision_model.vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'vision_model.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'vision_model.vision_model.encoder.layers.6.self_attn.out_proj.bias', 'vision_model.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'vision_model.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'vision_model.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'vision_model.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'vision_model.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'vision_model.vision_model.encoder.layers.7.layer_norm1.bias', 'vision_model.vision_model.encoder.layers.7.layer_norm1.weight', 'vision_model.vision_model.encoder.layers.7.layer_norm2.bias', 'vision_model.vision_model.encoder.layers.7.layer_norm2.weight', 'vision_model.vision_model.encoder.layers.7.mlp.fc1.bias', 'vision_model.vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.vision_model.encoder.layers.7.mlp.fc2.bias', 'vision_model.vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'vision_model.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'vision_model.vision_model.encoder.layers.7.self_attn.out_proj.bias', 'vision_model.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'vision_model.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'vision_model.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'vision_model.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'vision_model.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'vision_model.vision_model.encoder.layers.8.layer_norm1.bias', 'vision_model.vision_model.encoder.layers.8.layer_norm1.weight', 'vision_model.vision_model.encoder.layers.8.layer_norm2.bias', 'vision_model.vision_model.encoder.layers.8.layer_norm2.weight', 'vision_model.vision_model.encoder.layers.8.mlp.fc1.bias', 'vision_model.vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.vision_model.encoder.layers.8.mlp.fc2.bias', 'vision_model.vision_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'vision_model.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'vision_model.vision_model.encoder.layers.8.self_attn.out_proj.bias', 'vision_model.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'vision_model.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'vision_model.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'vision_model.vision_model.encoder.layers.8.self_attn.v_proj.bias', 'vision_model.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'vision_model.vision_model.encoder.layers.9.layer_norm1.bias', 'vision_model.vision_model.encoder.layers.9.layer_norm1.weight', 'vision_model.vision_model.encoder.layers.9.layer_norm2.bias', 'vision_model.vision_model.encoder.layers.9.layer_norm2.weight', 'vision_model.vision_model.encoder.layers.9.mlp.fc1.bias', 'vision_model.vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.vision_model.encoder.layers.9.mlp.fc2.bias', 'vision_model.vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'vision_model.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'vision_model.vision_model.encoder.layers.9.self_attn.out_proj.bias', 'vision_model.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'vision_model.vision_model.encoder.layers.9.self_attn.q_proj.bias', 'vision_model.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'vision_model.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'vision_model.vision_model.encoder.layers.9.self_attn.v_proj.weight', 'vision_model.vision_model.head.attention.in_proj_bias', 'vision_model.vision_model.head.attention.in_proj_weight', 'vision_model.vision_model.head.attention.out_proj.bias', 'vision_model.vision_model.head.attention.out_proj.weight', 'vision_model.vision_model.head.layernorm.bias', 'vision_model.vision_model.head.layernorm.weight', 'vision_model.vision_model.head.mlp.fc1.bias', 'vision_model.vision_model.head.mlp.fc1.weight', 'vision_model.vision_model.head.mlp.fc2.bias', 'vision_model.vision_model.head.mlp.fc2.weight', 'vision_model.vision_model.head.probe', 'vision_model.vision_model.post_layernorm.bias', 'vision_model.vision_model.post_layernorm.weight']\n- This IS expected if you are initializing JasperVL from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing JasperVL from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.72k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a286b837395e4ce882913c249c29fbda"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"95f8c7a50ba64ceeb0949da4d0f52b7c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c7da480128b748adb8f42efcad9aaeb4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a9e4c0df506340caa97f5667ec22624d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/180 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"18f98317f1ff4f868091ae64ae731b1a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/370 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"905fe838a4c6454895773bade72735e0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/394 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"793b02a33de24222bf5fca6b71c27050"}},"metadata":{}}],"execution_count":52},{"cell_type":"code","source":"def find_target_modules(model):\n    # Initialize a Set to Store Unique Layers\n    unique_layers = set()\n\n    # Iterate Over All Named Modules in the Model\n    for name, module in model.named_modules():\n        # Check if the Module Type Contains 'Linear4bit'\n        if \"Linear4bit\" in str(type(module)):\n            # Extract the Type of the Layer\n            layer_type = name.split('.')[-1]\n\n            # Add the Layer Type to the Set of Unique Layers\n            unique_layers.add(layer_type)\n\n    # Return the Set of Unique Layers Converted to a List\n    return list(unique_layers)\nfind_target_modules(embedding_model)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2025-01-08T06:28:47.730048Z","iopub.execute_input":"2025-01-08T06:28:47.730338Z","iopub.status.idle":"2025-01-08T06:28:47.737216Z","shell.execute_reply.started":"2025-01-08T06:28:47.730309Z","shell.execute_reply":"2025-01-08T06:28:47.736463Z"},"id":"h5TGLlXVBaeB","outputId":"6a0d572d-dfc7-4c1f-ba76-020f63797e94","trusted":true},"outputs":[{"execution_count":53,"output_type":"execute_result","data":{"text/plain":"[]"},"metadata":{}}],"execution_count":53},{"cell_type":"code","source":"# a=","metadata":{"execution":{"iopub.status.busy":"2025-01-08T06:28:47.738174Z","iopub.execute_input":"2025-01-08T06:28:47.738484Z","iopub.status.idle":"2025-01-08T06:28:47.756564Z","shell.execute_reply.started":"2025-01-08T06:28:47.738454Z","shell.execute_reply":"2025-01-08T06:28:47.755904Z"},"id":"AmPLSuxxBaeB","trusted":true},"outputs":[],"execution_count":54},{"cell_type":"code","source":"embedding_model.eval() # embedding_model in training mode (dropout modules are activated)\n\nif USE_LORA:\n\n  # enable gradient check pointing\n\n  embedding_model.gradient_checkpointing_enable()\n\n  target_modules_dict={\n\n      \"microsoft/Phi-3.5-mini-instruct\":'all-linear',\n      \"unsloth/Llama-3.2-1B-Instruct\":[\"q_proj\",\"k_proj\",\"v_proj\"],\n      \"unsloth/Qwen2.5-3B-Instruct-bnb-4bit\":[\"q_proj\",\"k_proj\",\"v_proj\"],\n  }\n\n  target_modules= target_modules_dict.get(x,[\"q_proj\"])\n\n\n\n  # enable quantized training\n\n  embedding_model = prepare_model_for_kbit_training(embedding_model)\n\n  config = LoraConfig(\n\n    r=8,\n\n    lora_alpha=32,\n\n    # target_modules=[\"k_proj\"],\n\n    target_modules = target_modules,#TODOOOOOOOOOOOOO\n\n    lora_dropout=0.1,\n\n    bias=\"none\",\n\n    task_type=\"SEQ_CLS\",\n\n  )\n\n  # LoRA trainable version of model\n\n  embedding_model = get_peft_model(embedding_model, config)\n\n\n\n  # trainable parameter count\n\n  embedding_model.print_trainable_parameters()\n\ndevice=\"cuda\" if torch.cuda.is_available() else 'cpu'","metadata":{"execution":{"iopub.status.busy":"2025-01-08T06:28:47.757288Z","iopub.execute_input":"2025-01-08T06:28:47.757492Z","iopub.status.idle":"2025-01-08T06:28:47.771165Z","shell.execute_reply.started":"2025-01-08T06:28:47.757474Z","shell.execute_reply":"2025-01-08T06:28:47.770503Z"},"id":"MCHCmWeqZd1R","trusted":true},"outputs":[],"execution_count":55},{"cell_type":"code","source":"device","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"execution":{"iopub.status.busy":"2025-01-08T06:28:47.771951Z","iopub.execute_input":"2025-01-08T06:28:47.772195Z","iopub.status.idle":"2025-01-08T06:28:47.787577Z","shell.execute_reply.started":"2025-01-08T06:28:47.772175Z","shell.execute_reply":"2025-01-08T06:28:47.786932Z"},"id":"CWtjrTGfJtvS","outputId":"055dcbf5-841f-4b31-c2e1-7d6b7dc3e33e","trusted":true},"outputs":[{"execution_count":56,"output_type":"execute_result","data":{"text/plain":"'cuda'"},"metadata":{}}],"execution_count":56},{"cell_type":"code","source":"embedding_model = embedding_model.to(device)\n\nfor param in embedding_model.parameters():\n    param.data = param.data.contiguous()\n","metadata":{"execution":{"iopub.status.busy":"2025-01-08T06:28:47.788272Z","iopub.execute_input":"2025-01-08T06:28:47.788512Z","iopub.status.idle":"2025-01-08T06:28:47.806270Z","shell.execute_reply.started":"2025-01-08T06:28:47.788481Z","shell.execute_reply":"2025-01-08T06:28:47.805597Z"},"id":"Ot09dhy1Jq2T","trusted":true},"outputs":[],"execution_count":57},{"cell_type":"code","source":"\n# Custom Dataset for batching\nclass TextDataset(Dataset):\n    def __init__(self, texts):\n        self.texts = texts\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        # Tokenize a single example\n        return self.texts[idx]\n        # return self.tokenizer(\n        #     self.texts[idx],\n        #     truncation=True,\n        #     padding=\"max_length\",\n        #     max_length=self.max_length,\n        #     return_tensors=\"pt\"\n        # )\n","metadata":{"id":"LcyDYbHFFhS4","trusted":true,"execution":{"iopub.status.busy":"2025-01-08T06:28:47.807084Z","iopub.execute_input":"2025-01-08T06:28:47.807334Z","iopub.status.idle":"2025-01-08T06:28:47.811898Z","shell.execute_reply.started":"2025-01-08T06:28:47.807314Z","shell.execute_reply":"2025-01-08T06:28:47.811262Z"}},"outputs":[],"execution_count":58},{"cell_type":"code","source":"\n# Batch extraction function\ndef extract_embeddings(texts, model, batch_size=32):\n    dataset = TextDataset(texts)\n    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n\n    embeddings = []\n    model.eval()\n    with torch.no_grad():\n        for batch in tqdm(dataloader, desc=\"Extracting embeddings\"):\n            # Move batch to device\n            # input_ids = torch.Tensor(batch['input_ids']).squeeze().to(device)\n            # attention_mask = torch.Tensor(batch['attention_mask']).squeeze().to(device)\n\n            # Forward pass through the model\n            cls_embeddings = model.encode(batch)\n            embeddings.append(cls_embeddings)\n\n    return np.vstack(embeddings)","metadata":{"id":"s8dV8g_NDo_v","trusted":true,"execution":{"iopub.status.busy":"2025-01-08T06:28:47.812791Z","iopub.execute_input":"2025-01-08T06:28:47.813072Z","iopub.status.idle":"2025-01-08T06:28:47.830671Z","shell.execute_reply.started":"2025-01-08T06:28:47.813044Z","shell.execute_reply":"2025-01-08T06:28:47.830018Z"}},"outputs":[],"execution_count":59},{"cell_type":"code","source":"# Extract embeddings for train and validation datasets\nx_train_embeddings = extract_embeddings(x_train, embedding_model, batch_size=32)\nx_valid_embeddings = extract_embeddings(x_valid, embedding_model, batch_size=32)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QkJtUXXiDx22","outputId":"4dd2c3dc-a3c3-46a7-f730-86e8b74afc0c","trusted":true,"execution":{"iopub.status.busy":"2025-01-08T06:28:47.831396Z","iopub.execute_input":"2025-01-08T06:28:47.831666Z","iopub.status.idle":"2025-01-08T06:29:19.804354Z","shell.execute_reply.started":"2025-01-08T06:28:47.831642Z","shell.execute_reply":"2025-01-08T06:29:19.803678Z"}},"outputs":[{"name":"stderr","text":"Extracting embeddings:   0%|          | 0/31 [00:00<?, ?it/s]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"47b92961a1fc4f12a038d34b30fe20e6"}},"metadata":{}},{"name":"stderr","text":"Extracting embeddings:   3%|▎         | 1/31 [00:01<00:38,  1.29s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3c6dac5238144387b014db5421849a1b"}},"metadata":{}},{"name":"stderr","text":"Extracting embeddings:   6%|▋         | 2/31 [00:02<00:30,  1.04s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"06bbf29968d84e6eb985c7a81abec74e"}},"metadata":{}},{"name":"stderr","text":"Extracting embeddings:  10%|▉         | 3/31 [00:02<00:25,  1.11it/s]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5baaaca752734a328c2e334ccb731f09"}},"metadata":{}},{"name":"stderr","text":"Extracting embeddings:  13%|█▎        | 4/31 [00:03<00:23,  1.17it/s]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4b26f544c5404df2a9468156faae2aad"}},"metadata":{}},{"name":"stderr","text":"Extracting embeddings:  16%|█▌        | 5/31 [00:04<00:22,  1.15it/s]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ceb2ebf6d4394f48a52c13f2b8891aa8"}},"metadata":{}},{"name":"stderr","text":"Extracting embeddings:  19%|█▉        | 6/31 [00:05<00:22,  1.14it/s]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"93edecccf6a94711abe641a1db8e4c85"}},"metadata":{}},{"name":"stderr","text":"Extracting embeddings:  23%|██▎       | 7/31 [00:06<00:21,  1.14it/s]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1227de8a3363417d860adc73cd190ae0"}},"metadata":{}},{"name":"stderr","text":"Extracting embeddings:  26%|██▌       | 8/31 [00:07<00:20,  1.15it/s]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7aebdc87f7054a659ce585fa35783384"}},"metadata":{}},{"name":"stderr","text":"Extracting embeddings:  29%|██▉       | 9/31 [00:08<00:18,  1.17it/s]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"284fb621a4e341e9a8b70fa707a3e060"}},"metadata":{}},{"name":"stderr","text":"Extracting embeddings:  32%|███▏      | 10/31 [00:08<00:17,  1.21it/s]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dbe6f166215541aea9316a9d08264669"}},"metadata":{}},{"name":"stderr","text":"Extracting embeddings:  35%|███▌      | 11/31 [00:09<00:16,  1.20it/s]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"93b9d1624c1649868378dd84214ab8f8"}},"metadata":{}},{"name":"stderr","text":"Extracting embeddings:  39%|███▊      | 12/31 [00:10<00:15,  1.19it/s]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"64219c1bf9d34aad88d9a86ba5a3619f"}},"metadata":{}},{"name":"stderr","text":"Extracting embeddings:  42%|████▏     | 13/31 [00:11<00:15,  1.13it/s]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"69b712946cab400897ef50024f33cfda"}},"metadata":{}},{"name":"stderr","text":"Extracting embeddings:  45%|████▌     | 14/31 [00:12<00:14,  1.15it/s]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8802094421ba4515a806e39444d58539"}},"metadata":{}},{"name":"stderr","text":"Extracting embeddings:  48%|████▊     | 15/31 [00:13<00:13,  1.16it/s]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"06e051c5179d463c8a1433d392b22d9a"}},"metadata":{}},{"name":"stderr","text":"Extracting embeddings:  52%|█████▏    | 16/31 [00:13<00:12,  1.21it/s]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"efa0a58ac33644beb6f98b5f556ca715"}},"metadata":{}},{"name":"stderr","text":"Extracting embeddings:  55%|█████▍    | 17/31 [00:14<00:11,  1.22it/s]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2638e0d4baa84e4b9ea867792752dd8a"}},"metadata":{}},{"name":"stderr","text":"Extracting embeddings:  58%|█████▊    | 18/31 [00:15<00:10,  1.25it/s]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f7b92232d7314f8499c48df149669038"}},"metadata":{}},{"name":"stderr","text":"Extracting embeddings:  61%|██████▏   | 19/31 [00:16<00:09,  1.25it/s]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c8cede9006ed46b7a152266db77e180d"}},"metadata":{}},{"name":"stderr","text":"Extracting embeddings:  65%|██████▍   | 20/31 [00:17<00:08,  1.23it/s]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7847f406d98b4cacb7103fd8b4a4f5a0"}},"metadata":{}},{"name":"stderr","text":"Extracting embeddings:  68%|██████▊   | 21/31 [00:17<00:08,  1.22it/s]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f4151ee25c4447799f1448871998f8d4"}},"metadata":{}},{"name":"stderr","text":"Extracting embeddings:  71%|███████   | 22/31 [00:18<00:07,  1.21it/s]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"26fb766fa5ec40b483ccc52b7b8936be"}},"metadata":{}},{"name":"stderr","text":"Extracting embeddings:  74%|███████▍  | 23/31 [00:19<00:06,  1.22it/s]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0d5bd2e2efb04b769b61b24e745bb307"}},"metadata":{}},{"name":"stderr","text":"Extracting embeddings:  77%|███████▋  | 24/31 [00:20<00:05,  1.26it/s]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"72f731efc2d74deeadc9f9c5bae98c7e"}},"metadata":{}},{"name":"stderr","text":"Extracting embeddings:  81%|████████  | 25/31 [00:21<00:04,  1.26it/s]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"57affbf81ceb4e6ead06cebb6faf0fe6"}},"metadata":{}},{"name":"stderr","text":"Extracting embeddings:  84%|████████▍ | 26/31 [00:21<00:03,  1.26it/s]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e9941b7cf51c4cd5bf9930a5b3648ebe"}},"metadata":{}},{"name":"stderr","text":"Extracting embeddings:  87%|████████▋ | 27/31 [00:22<00:03,  1.28it/s]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b4f00c8f69ec45878fb622ec5cd6fb76"}},"metadata":{}},{"name":"stderr","text":"Extracting embeddings:  90%|█████████ | 28/31 [00:23<00:02,  1.27it/s]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"97441bd5dc7c45788235646c95eb58de"}},"metadata":{}},{"name":"stderr","text":"Extracting embeddings:  94%|█████████▎| 29/31 [00:24<00:01,  1.28it/s]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7523b910e1e249df91a61744ebfdf7aa"}},"metadata":{}},{"name":"stderr","text":"Extracting embeddings:  97%|█████████▋| 30/31 [00:24<00:00,  1.28it/s]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3cccbc63e4964da79f53a7be604131c4"}},"metadata":{}},{"name":"stderr","text":"Extracting embeddings: 100%|██████████| 31/31 [00:25<00:00,  1.21it/s]\nExtracting embeddings:   0%|          | 0/8 [00:00<?, ?it/s]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6a1f80826a314e1d912c059d0dd954e3"}},"metadata":{}},{"name":"stderr","text":"Extracting embeddings:  12%|█▎        | 1/8 [00:00<00:06,  1.13it/s]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fe42424ebaf44a2ebd0ce922e8eb6fe7"}},"metadata":{}},{"name":"stderr","text":"Extracting embeddings:  25%|██▌       | 2/8 [00:01<00:05,  1.17it/s]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8d790168c24340da8cefcec632bc596b"}},"metadata":{}},{"name":"stderr","text":"Extracting embeddings:  38%|███▊      | 3/8 [00:02<00:04,  1.21it/s]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e69ea3427c064916b3755e24116bdca4"}},"metadata":{}},{"name":"stderr","text":"Extracting embeddings:  50%|█████     | 4/8 [00:03<00:03,  1.19it/s]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2f981851e73d4ba0ae6116fdea2973d9"}},"metadata":{}},{"name":"stderr","text":"Extracting embeddings:  62%|██████▎   | 5/8 [00:04<00:02,  1.22it/s]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ae47624110c44a5fa7e4b340d05ae6dd"}},"metadata":{}},{"name":"stderr","text":"Extracting embeddings:  75%|███████▌  | 6/8 [00:04<00:01,  1.21it/s]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4fa2eeb57b874b4fbcbe51d97b85200d"}},"metadata":{}},{"name":"stderr","text":"Extracting embeddings:  88%|████████▊ | 7/8 [00:05<00:00,  1.20it/s]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"16e264aa456a422f99214085bb1116a3"}},"metadata":{}},{"name":"stderr","text":"Extracting embeddings: 100%|██████████| 8/8 [00:06<00:00,  1.25it/s]\n","output_type":"stream"}],"execution_count":60},{"cell_type":"code","source":"# Flatten labels for XGBoost (multi-label classification)\ny_train_flat = np.array(y_train)\ny_valid_flat = np.array(y_valid)","metadata":{"id":"J-d1ZAwbDzS-","trusted":true,"execution":{"iopub.status.busy":"2025-01-08T06:29:19.805133Z","iopub.execute_input":"2025-01-08T06:29:19.805410Z","iopub.status.idle":"2025-01-08T06:29:19.809604Z","shell.execute_reply.started":"2025-01-08T06:29:19.805374Z","shell.execute_reply":"2025-01-08T06:29:19.808749Z"}},"outputs":[],"execution_count":61},{"cell_type":"code","source":"import xgboost as xgb","metadata":{"id":"TMRlx05eKvBY","trusted":true,"execution":{"iopub.status.busy":"2025-01-08T06:29:19.810499Z","iopub.execute_input":"2025-01-08T06:29:19.810845Z","iopub.status.idle":"2025-01-08T06:29:20.066867Z","shell.execute_reply.started":"2025-01-08T06:29:19.810814Z","shell.execute_reply":"2025-01-08T06:29:20.066216Z"}},"outputs":[],"execution_count":62},{"cell_type":"code","source":"xgb_models = {}\npredictions = {}\n\nfor i, label in enumerate(label_columns):\n    print(f\"Training XGBoost for label: {label}\")\n    # Use the computed class weights for the positive class\n    scale_pos_weight = class_weights_tensor[i].item()\n    # Initialize and train XGBoost classifier\n    xgb_model = xgb.XGBClassifier(\n        objective='binary:logistic',\n        eval_metric='logloss',\n        use_label_encoder=False,\n        n_estimators=100,\n        learning_rate=0.1,\n        max_depth=6,\n        tree_method='hist',\n        scale_pos_weight=scale_pos_weight,  # Apply the class weight here\n        device=device\n    )\n    xgb_model.fit(x_train_embeddings, y_train_flat[:, i])\n    xgb_models[label] = xgb_model\n\n    # Predict on validation data\n    predictions[label] = xgb_model.predict(x_valid_embeddings)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lEP0kNeYD7L2","outputId":"a74017f0-6a8e-41b1-e31e-45ed69706f71","trusted":true,"execution":{"iopub.status.busy":"2025-01-08T06:29:20.067572Z","iopub.execute_input":"2025-01-08T06:29:20.067773Z","iopub.status.idle":"2025-01-08T06:29:22.716540Z","shell.execute_reply.started":"2025-01-08T06:29:20.067754Z","shell.execute_reply":"2025-01-08T06:29:22.715813Z"}},"outputs":[{"name":"stdout","text":"Training XGBoost for label: anger\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [06:29:20] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\nPotential solutions:\n- Use a data structure that matches the device ordinal in the booster.\n- Set the device for booster before call to inplace_predict.\n\nThis warning will only be shown once.\n\n  warnings.warn(smsg, UserWarning)\n","output_type":"stream"},{"name":"stdout","text":"Training XGBoost for label: disgust\nTraining XGBoost for label: fear\nTraining XGBoost for label: joy\nTraining XGBoost for label: sadness\n","output_type":"stream"}],"execution_count":63},{"cell_type":"code","source":"# Evaluate the models\nresults = []\nfor i, label in enumerate(label_columns):\n    precision, recall, f1, _ = precision_recall_fscore_support(\n        y_valid_flat[:, i],\n        predictions[label],\n        average=\"binary\"\n    )\n    accuracy = accuracy_score(y_valid_flat[:, i], predictions[label])\n    results.append({\n        \"Label\": label,\n        \"Precision\": precision,\n        \"Recall\": recall,\n        \"F1 Score\": f1,\n        \"Accuracy\": accuracy\n    })","metadata":{"id":"71wf1rQCD9tn","trusted":true,"execution":{"iopub.status.busy":"2025-01-08T06:29:22.717279Z","iopub.execute_input":"2025-01-08T06:29:22.717580Z","iopub.status.idle":"2025-01-08T06:29:22.735843Z","shell.execute_reply.started":"2025-01-08T06:29:22.717559Z","shell.execute_reply":"2025-01-08T06:29:22.735109Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"}],"execution_count":64},{"cell_type":"code","source":"# Display evaluation results\nresults_df = pd.DataFrame(results)\nprint(results_df)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7mxcaQ7dEBZm","outputId":"a58b090f-dc9f-48c9-f1ff-998a26cfe148","trusted":true,"execution":{"iopub.status.busy":"2025-01-08T06:29:22.736869Z","iopub.execute_input":"2025-01-08T06:29:22.737163Z","iopub.status.idle":"2025-01-08T06:29:22.745996Z","shell.execute_reply.started":"2025-01-08T06:29:22.737135Z","shell.execute_reply":"2025-01-08T06:29:22.745268Z"}},"outputs":[{"name":"stdout","text":"     Label  Precision    Recall  F1 Score  Accuracy\n0    anger   0.000000  0.000000  0.000000  0.967347\n1  disgust   0.000000  0.000000  0.000000  0.987755\n2     fear   0.250000  0.043478  0.074074  0.897959\n3      joy   0.627907  0.519231  0.568421  0.665306\n4  sadness   0.500000  0.065217  0.115385  0.812245\n","output_type":"stream"}],"execution_count":65},{"cell_type":"code","source":"print(f\"{x} evaluates:\")\n\nfor k, res in enumerate(results):\n\n  print(f\"Class '{res['Label']}': Precision: {res['Precision']:.4f}, Recall: {res['Recall']:.4f}, F1: {res['F1 Score']:.4f}\")\n\nprint(f\"Overall accuracy: {accuracy:.4f}\")\n\nprint(\"-------------------------------------------------------------------------\\n\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2025-01-08T06:29:22.746809Z","iopub.execute_input":"2025-01-08T06:29:22.747058Z","iopub.status.idle":"2025-01-08T06:29:22.762029Z","shell.execute_reply.started":"2025-01-08T06:29:22.747028Z","shell.execute_reply":"2025-01-08T06:29:22.761084Z"},"id":"uGLXR8HmBaeC","outputId":"d62c77d7-2ffe-469c-823c-be89cf9d433d","trusted":true},"outputs":[{"name":"stdout","text":"infgrad/jasper_en_vision_language_v1 evaluates:\nClass 'anger': Precision: 0.0000, Recall: 0.0000, F1: 0.0000\nClass 'disgust': Precision: 0.0000, Recall: 0.0000, F1: 0.0000\nClass 'fear': Precision: 0.2500, Recall: 0.0435, F1: 0.0741\nClass 'joy': Precision: 0.6279, Recall: 0.5192, F1: 0.5684\nClass 'sadness': Precision: 0.5000, Recall: 0.0652, F1: 0.1154\nOverall accuracy: 0.8122\n-------------------------------------------------------------------------\n\n","output_type":"stream"}],"execution_count":66},{"cell_type":"code","source":"# Save the XGBoost models if needed\nfor label, model in xgb_models.items():\n    model.save_model(f\"xgb_model_{label}.json\")","metadata":{"id":"NabI3knjECpg","trusted":true,"execution":{"iopub.status.busy":"2025-01-08T06:29:22.762997Z","iopub.execute_input":"2025-01-08T06:29:22.763275Z","iopub.status.idle":"2025-01-08T06:29:22.796649Z","shell.execute_reply.started":"2025-01-08T06:29:22.763245Z","shell.execute_reply":"2025-01-08T06:29:22.796012Z"}},"outputs":[],"execution_count":67},{"cell_type":"code","source":"# Initialize a DataFrame to store the evaluation results for each language\nlang_metrics_df = pd.DataFrame(columns=[\"Language\", \"Precision\", \"Recall\", \"F1_score\", \"Accuracy\"])\n\n# Iterate over each language\nfor lang in langs:\n    # Filter the validation data for the current language\n    lang_valid_data = valid[valid['lang'] == lang]\n    lang_x_valid = lang_valid_data['clean_message'].values.tolist()\n    lang_y_valid = lang_valid_data[label_columns].values.tolist()\n\n    if len(lang_valid_data) == 0:\n        continue\n\n    # Extract embeddings for the current language's validation data\n    lang_x_valid_embeddings = extract_embeddings(lang_x_valid, embedding_model)\n    print()\n    # Initialize metrics storage\n    all_precisions, all_recalls, all_f1_scores, all_accuracies = [], [], [], []\n\n    # Evaluate each label separately\n    for i, label in enumerate(label_columns):\n        # Predict using the corresponding XGBoost model\n        lang_predictions = xgb_models[label].predict(lang_x_valid_embeddings)\n\n        # Compute metrics\n        precision, recall, f1, _ = precision_recall_fscore_support(\n            np.array(lang_y_valid)[:, i],  # True labels for this label\n            lang_predictions,\n            average=\"binary\",\n            zero_division=0\n        )\n        accuracy = accuracy_score(np.array(lang_y_valid)[:, i], lang_predictions)\n\n        # Store metrics\n        all_precisions.append(precision)\n        all_recalls.append(recall)\n        all_f1_scores.append(f1)\n        all_accuracies.append(accuracy)\n\n    # Aggregate metrics (average across all labels for the language)\n    new_data = pd.DataFrame({\n        \"Language\": [lang],\n        \"Precision\": [np.mean(all_precisions)],\n        \"Recall\": [np.mean(all_recalls)],\n        \"F1_score\": [np.mean(all_f1_scores)],\n        \"Accuracy\": [np.mean(all_accuracies)]\n    })\n\n    lang_metrics_df = pd.concat([lang_metrics_df, new_data], ignore_index=True)\n\n# Display the evaluation results for each language\nprint(lang_metrics_df)\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2025-01-08T06:29:22.797600Z","iopub.execute_input":"2025-01-08T06:29:22.797881Z","iopub.status.idle":"2025-01-08T06:29:29.286429Z","shell.execute_reply.started":"2025-01-08T06:29:22.797849Z","shell.execute_reply":"2025-01-08T06:29:29.285485Z"},"id":"nNQrEWEJBaeC","outputId":"e697c224-2e22-4d15-fca8-392de10c4a7e","trusted":true},"outputs":[{"name":"stderr","text":"Extracting embeddings:   0%|          | 0/8 [00:00<?, ?it/s]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b8c6a372e56141cdbbc6eec7dbf33bf7"}},"metadata":{}},{"name":"stderr","text":"Extracting embeddings:  12%|█▎        | 1/8 [00:00<00:06,  1.12it/s]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c68677a57cdd4b40b225c3b7e5071ae1"}},"metadata":{}},{"name":"stderr","text":"Extracting embeddings:  25%|██▌       | 2/8 [00:01<00:05,  1.17it/s]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8befe239b6de43bfa1db15bbb64efe8f"}},"metadata":{}},{"name":"stderr","text":"Extracting embeddings:  38%|███▊      | 3/8 [00:02<00:04,  1.22it/s]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a2205dda9f0d4437b7585a845a63d840"}},"metadata":{}},{"name":"stderr","text":"Extracting embeddings:  50%|█████     | 4/8 [00:03<00:03,  1.20it/s]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"777ebdf5c05242f690971b31e38f1f4c"}},"metadata":{}},{"name":"stderr","text":"Extracting embeddings:  62%|██████▎   | 5/8 [00:04<00:02,  1.22it/s]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0e93beb135eb40eba14f60edb6e4cd01"}},"metadata":{}},{"name":"stderr","text":"Extracting embeddings:  75%|███████▌  | 6/8 [00:04<00:01,  1.21it/s]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b65e609ca66f4469a12db90b3d58d3a4"}},"metadata":{}},{"name":"stderr","text":"Extracting embeddings:  88%|████████▊ | 7/8 [00:05<00:00,  1.19it/s]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fe5c5339286d468187d103eaa758c340"}},"metadata":{}},{"name":"stderr","text":"Extracting embeddings: 100%|██████████| 8/8 [00:06<00:00,  1.24it/s]","output_type":"stream"},{"name":"stdout","text":"\n  Language  Precision    Recall  F1_score  Accuracy\n0      afr   0.275581  0.125585  0.151576  0.866122\n","output_type":"stream"},{"name":"stderr","text":"\n<ipython-input-68-a88114e91a2a>:49: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n  lang_metrics_df = pd.concat([lang_metrics_df, new_data], ignore_index=True)\n","output_type":"stream"}],"execution_count":68}]}