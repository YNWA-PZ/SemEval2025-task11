{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rVRSZaZ7DdeQ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "\n",
        "os.environ['WANDB_DISABLED'] = \"true\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-01-05T19:15:21.336921Z",
          "iopub.status.busy": "2025-01-05T19:15:21.336601Z",
          "iopub.status.idle": "2025-01-05T19:15:28.642730Z",
          "shell.execute_reply": "2025-01-05T19:15:28.641846Z",
          "shell.execute_reply.started": "2025-01-05T19:15:21.336883Z"
        },
        "id": "jiRg5MiCozSJ",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "!gdown 1EI12smTmdi4Mlu_lLNULN6p6g9dYEWql"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-01-05T19:15:28.644379Z",
          "iopub.status.busy": "2025-01-05T19:15:28.644090Z",
          "iopub.status.idle": "2025-01-05T19:15:29.037021Z",
          "shell.execute_reply": "2025-01-05T19:15:29.035964Z",
          "shell.execute_reply.started": "2025-01-05T19:15:28.644356Z"
        },
        "id": "kexrENAEo54E",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "!rm -rf processed_data\n",
        "\n",
        "!unzip -q processed_data.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CVEiWrCU2mAy"
      },
      "outputs": [],
      "source": [
        "!git config --global credential.helper store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MdRjss8TFK-6"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "# Log in using your Hugging Face token\n",
        "login(token=\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8a94O5AdFK-6"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import Repository\n",
        "\n",
        "# Replace with your repo URL\n",
        "repo_url = \"https://huggingface.co/ErfanSadegh/SemEval2025-Task11-TrackA\"\n",
        "\n",
        "# Specify the local directory where the repo will be cloned\n",
        "local_dir = \"SemEval2025-Task11-TrackA\"\n",
        "\n",
        "# Clone the repository\n",
        "repo = Repository(local_dir=local_dir, clone_from=repo_url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-01-05T19:15:29.038684Z",
          "iopub.status.busy": "2025-01-05T19:15:29.038337Z",
          "iopub.status.idle": "2025-01-05T19:16:03.984740Z",
          "shell.execute_reply": "2025-01-05T19:16:03.983693Z",
          "shell.execute_reply.started": "2025-01-05T19:15:29.038659Z"
        },
        "id": "VC2TifwamSFK",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers[torch] accelerate -U\n",
        "!pip install -q sentence_transformers\n",
        "\n",
        "!pip install -q datasets\n",
        "\n",
        "!pip install -q transformers\n",
        "\n",
        "!pip install -q iterative-stratification\n",
        "\n",
        "!pip install -q auto-gptq optimum bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-01-05T19:16:03.986156Z",
          "iopub.status.busy": "2025-01-05T19:16:03.985858Z",
          "iopub.status.idle": "2025-01-05T19:16:17.769314Z",
          "shell.execute_reply": "2025-01-05T19:16:17.768671Z",
          "shell.execute_reply.started": "2025-01-05T19:16:03.986133Z"
        },
        "id": "a_v9ZQjVoIma",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "import os\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "from transformers import BitsAndBytesConfig\n",
        "\n",
        "from torch.utils.data import DataLoader,Dataset\n",
        "\n",
        "import torch\n",
        "\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "import pickle\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "import xgboost as xgb\n",
        "\n",
        "from sklearn.svm import SVC\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-01-05T19:16:17.794172Z",
          "iopub.status.busy": "2025-01-05T19:16:17.793838Z",
          "iopub.status.idle": "2025-01-05T19:16:17.808123Z",
          "shell.execute_reply": "2025-01-05T19:16:17.807421Z",
          "shell.execute_reply.started": "2025-01-05T19:16:17.794136Z"
        },
        "id": "1kwye6PzpT4N",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "MAX_LENGTH = 512\n",
        "# kind=\"dev\"\n",
        "# task=\"track_c\"\n",
        "\n",
        "# langs=[\"afr\",\"amh\",\"deu\",\"eng\",\"oro\",\"ptbr\",\"rus\",\"som\",\"sum\",\"tir\"]\n",
        "# labels=['Anger','Disgust', 'Fear', 'Joy', 'Sadness', 'Surprise']\n",
        "# langs=[\"amh\",\"arq\",\"ary\",\"chn\",\"deu\",\"esp\",\"hau\",\"hin\",\"ibo\",\"kin\",\"mar\",\"orm\",\"pcm\",\"ptbr\",\"ptmz\",\"ron\",\"rus\",\"som\",\"sun\",\"swa\",\"swe\",\"tat\",\"tir\",\"ukr\",\"vmw\",\"yor\",\"ind\",\"jav\",\"xho\",\"zul\"]\n",
        "# langs=[\"esp\"]\n",
        "# langs=[\"eng\"]\n",
        "# langs=[\"afr\"]\n",
        "\n",
        "# label_columns =['anger', 'fear', 'joy', 'sadness', 'surprise'] #eng\n",
        "label_columns =['anger', 'disgust', 'fear', 'joy', 'sadness', 'surprise']\n",
        "# label_columns =['anger', 'disgust', 'fear', 'joy', 'sadness'] #afr\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-01-05T19:16:17.809100Z",
          "iopub.status.busy": "2025-01-05T19:16:17.808821Z",
          "iopub.status.idle": "2025-01-05T19:16:17.843638Z",
          "shell.execute_reply": "2025-01-05T19:16:17.842877Z",
          "shell.execute_reply.started": "2025-01-05T19:16:17.809071Z"
        },
        "id": "1x0MboqJuj21",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def read_csv(tasks,kinds,langs):\n",
        "  all_csv=pd.DataFrame()\n",
        "  for task in tasks:\n",
        "    for kind in kinds:\n",
        "      for lang in langs:\n",
        "        processed_path=f\"processed_data/{kind}/{task}/{lang}.csv\"\n",
        "        if not os.path.isfile(processed_path):\n",
        "\n",
        "          print(\"not found:\",processed_path)\n",
        "\n",
        "          continue\n",
        "        train_data=pd.read_csv(processed_path)\n",
        "\n",
        "        train_data.columns = train_data.columns.str.lower()\n",
        "\n",
        "        all_csv = pd.concat([all_csv, train_data],ignore_index=True)\n",
        "  if len(all_csv)==0:\n",
        "    print(f\"{lang} with {kinds} and {tasks} is empty\")\n",
        "    return None,None,None\n",
        "  train_data=all_csv\n",
        "  train_data=train_data.fillna(0)\n",
        "  float_columns = train_data.select_dtypes(include=['float64']).columns\n",
        "  # Convert those columns to int\n",
        "  train_data[float_columns] = train_data[float_columns].astype('int')\n",
        "  train_data = train_data[train_data['clean_message'].isnull()==False]\n",
        "\n",
        "  train_data.reset_index()\n",
        "  train_data['clean_message']=train_data['clean_message'].astype(str)\n",
        "  train_data=train_data.sample(frac=1)\n",
        "  x_train, y_train = train_data['clean_message'].values.tolist(), train_data[label_columns].values.tolist()\n",
        "  return train_data,x_train,y_train\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-01-05T19:16:18.224287Z",
          "iopub.status.busy": "2025-01-05T19:16:18.223981Z",
          "iopub.status.idle": "2025-01-05T19:16:18.281876Z",
          "shell.execute_reply": "2025-01-05T19:16:18.281035Z",
          "shell.execute_reply.started": "2025-01-05T19:16:18.224260Z"
        },
        "id": "-EFMbbzALDd6",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "device=\"cuda\" if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-01-05T19:16:18.205908Z",
          "iopub.status.busy": "2025-01-05T19:16:18.205671Z",
          "iopub.status.idle": "2025-01-05T19:16:18.223109Z",
          "shell.execute_reply": "2025-01-05T19:16:18.222373Z",
          "shell.execute_reply.started": "2025-01-05T19:16:18.205889Z"
        },
        "id": "sVD0DSPvmdcE",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#update label column name\n",
        "\n",
        "def get_class_weights_tensor(y_train):\n",
        "    y_train_df = pd.DataFrame(y_train, columns=label_columns)\n",
        "    label_distribution = y_train_df.apply(pd.Series.value_counts).T.fillna(0).astype(int)\n",
        "\n",
        "    label_distribution.columns = ['count_0', 'count_1']\n",
        "    label_distribution['sum'] = label_distribution['count_0'] + label_distribution['count_1']\n",
        "\n",
        "    counts_0 = label_distribution['count_0'].to_numpy()\n",
        "    counts_1 = label_distribution['count_1'].to_numpy()\n",
        "\n",
        "    # Compute class-specific weights for each label (each class)\n",
        "    class_weights = []\n",
        "    for i in range(len(counts_0)):\n",
        "        weight = compute_class_weight('balanced', classes=np.array([0, 1]), y=[0] * counts_0[i] + [1] * counts_1[i])\n",
        "        class_weights.append(weight)\n",
        "\n",
        "    # Convert the list of weights to a tensor\n",
        "    class_weights_tensor = torch.tensor([w[1] for w in class_weights], dtype=torch.float,device=device)\n",
        "    return class_weights_tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-01-05T19:16:18.996404Z",
          "iopub.status.busy": "2025-01-05T19:16:18.996116Z",
          "iopub.status.idle": "2025-01-05T19:16:19.009025Z",
          "shell.execute_reply": "2025-01-05T19:16:19.008387Z",
          "shell.execute_reply.started": "2025-01-05T19:16:18.996373Z"
        },
        "id": "3TcPpfq1GdUI",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "quantization_config = BitsAndBytesConfig(\n",
        "\n",
        "    load_in_4bit=True,\n",
        "\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-01-05T19:16:19.041374Z",
          "iopub.status.busy": "2025-01-05T19:16:19.041125Z",
          "iopub.status.idle": "2025-01-05T19:16:55.700389Z",
          "shell.execute_reply": "2025-01-05T19:16:55.699744Z",
          "shell.execute_reply.started": "2025-01-05T19:16:19.041354Z"
        },
        "id": "RTwQtNnRZXNI",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\n",
        "def load_model(model_name):\n",
        "    for _ in range(20):\n",
        "        torch.cuda.empty_cache()\n",
        "    print(model_name)\n",
        "    use_gpu = torch.cuda.is_available()\n",
        "    embedding_model = SentenceTransformer(\n",
        "            model_name,\n",
        "            trust_remote_code=True,\n",
        "            device=\"cpu\" if not use_gpu else \"cuda\",\n",
        "            model_kwargs={\n",
        "                \"torch_dtype\": torch.bfloat16 if use_gpu else torch.float32,\n",
        "                # \"attn_implementation\": \"sdpa\"\n",
        "            },\n",
        "            # config_kwargs={\"is_text_encoder\": True, \"vector_dim\": 1024},\n",
        "        )\n",
        "        # We can reduce the max_seq_length from the default of 2048 for faster encoding\n",
        "    embedding_model.max_seq_length = MAX_LENGTH\n",
        "    embedding_model.eval()\n",
        "    embedding_model = embedding_model.to(device)\n",
        "    for param in embedding_model.parameters():\n",
        "        param.data = param.data.contiguous()\n",
        "    return embedding_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LcyDYbHFFhS4"
      },
      "outputs": [],
      "source": [
        "# Custom Dataset for batching\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, texts):\n",
        "        self.texts = texts\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.texts[idx]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s8dV8g_NDo_v"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Batch extraction function\n",
        "def extract_embeddings(texts, model, batch_size=32):\n",
        "    dataset = TextDataset(texts)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    embeddings = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dataloader, desc=\"Extracting embeddings\"):\n",
        "            # Move batch to device\n",
        "            # input_ids = torch.Tensor(batch['input_ids']).squeeze().to(device)\n",
        "            # attention_mask = torch.Tensor(batch['attention_mask']).squeeze().to(device)\n",
        "\n",
        "            # Forward pass through the model\n",
        "            cls_embeddings = model.encode(batch)\n",
        "            embeddings.append(cls_embeddings)\n",
        "\n",
        "    return np.vstack(embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PM9xtjSiFK--"
      },
      "outputs": [],
      "source": [
        "def load_models(dir):\n",
        "    # Initialize an empty dictionary to store the loaded SVM models\n",
        "    models = {}\n",
        "\n",
        "    # Loop through the saved files and load the models\n",
        "    for filename in os.listdir(dir):  # List all files in the current directory\n",
        "        if filename.startswith(\"svm_model_\") and filename.endswith(\".pkl\"):\n",
        "            label = filename[len(\"svm_model_\"):-len(\".pkl\")]  # Extract the label from the filename\n",
        "            with open(os.path.join(dir,filename), 'rb') as f:\n",
        "                models[label] = pickle.load(f)\n",
        "        elif filename.startswith(\"xgb_model_\") and filename.endswith(\".pkl\"):\n",
        "            label = filename[len(\"xgb_model_\"):-len(\".pkl\")]\n",
        "            with open(os.path.join(dir,filename), 'rb') as f:\n",
        "                models[label] = pickle.load(f)\n",
        "    return models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GrEgMGQN2mA2"
      },
      "outputs": [],
      "source": [
        "def save_models(models,dir,isxgb=False):\n",
        "    # Loop through the models and save them\n",
        "    #remove everything ending with zip\n",
        "    for filename in os.listdir(dir):\n",
        "        if filename.endswith(\".zip\"):\n",
        "            os.remove(os.path.join(dir,filename))\n",
        "\n",
        "    for label, model in models.items():\n",
        "        if isxgb:\n",
        "            filename = f\"xgb_model_{label}.pkl\"\n",
        "        else:\n",
        "            filename = f\"svm_model_{label}.pkl\"\n",
        "        with open(os.path.join(dir,filename), 'wb') as f:\n",
        "            pickle.dump(model, f)\n",
        "    # zip these pickle files using python\n",
        "    if isxgb:\n",
        "        zip_file_name = \"xgb_models_pkl.zip\"\n",
        "    else:\n",
        "        zip_file_name = \"svm_models_pkl.zip\"\n",
        "    zip_file_path = os.path.join(dir, zip_file_name)\n",
        "    import zipfile\n",
        "    with zipfile.ZipFile(zip_file_path, 'w') as z:\n",
        "        for filename in os.listdir(dir):\n",
        "            if filename.endswith(\".pkl\"):\n",
        "                z.write(os.path.join(dir,filename), filename)\n",
        "    for filename in os.listdir(dir):\n",
        "        if filename.endswith(\".pkl\"):\n",
        "            os.remove(os.path.join(dir,filename))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D2t8ywcWFK--"
      },
      "outputs": [],
      "source": [
        "model2lang={\n",
        "    \"intfloat/multilingual-e5-large-instruct\":\n",
        "    [\"afr\",\"amh\",\"ary\",\"deu\",\"esp\",\"hin\",\"ibo\",\"kin\",\"mar\",\"orm\",\"pcm\",\"ptmz\",\"ron\",\"som\",\"sun\",\"swa\",\"swe\",\"tat\",\"tir\",\"yor\",\"ind\",\"jav\",\"xho\",\"zul\"],\n",
        "    \"Abdou/arabert-large-algerian\":[\"arq\"],\n",
        "    \"iampanda/zpoint_large_embedding_zh\":[\"chn\"],\n",
        "    \"infgrad/jasper_en_vision_language_v1\":[\"eng\"],\n",
        "    \"konstantindobler/xlm-roberta-base-focus-hausa\":[\"hau\"],\n",
        "    \"neuralmind/bert-large-portuguese-cased\":[\"ptbr\"],\n",
        "    \"ai-forever/FRIDA\":[\"rus\"],\n",
        "    \"jinaai/jina-embeddings-v3\":[\"ukr\",\"vmw\"],\n",
        "}\n",
        "allLangs=[]\n",
        "allModel=\"intfloat/multilingual-e5-large-instruct\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b7cK5xegFK--"
      },
      "outputs": [],
      "source": [
        "lang2labelcolumns={\n",
        "    \"eng\":['anger', 'fear', 'joy', 'sadness', 'surprise'],\n",
        "    \"afr\":['anger', 'disgust', 'fear', 'joy', 'sadness'],\n",
        "    \"others\":['anger', 'disgust', 'fear', 'joy', 'sadness', 'surprise']\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o-EE0Ss0JsiD"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import zipfile\n",
        "\n",
        "def unzip_models(local_dir, langpath,zip_file_name=\"svm_models_pkl.zip\"):\n",
        "    zip_file_path = os.path.join(local_dir, \"models\", langpath, zip_file_name)\n",
        "    extract_dir = os.path.join(local_dir, \"models\", langpath)\n",
        "\n",
        "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "      for member in zip_ref.namelist():\n",
        "        zip_ref.extract(member, extract_dir)\n",
        "\n",
        "    # print(f\"Extracted {zip_file_path} to {extract_dir}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pFW7c6-fFK--"
      },
      "outputs": [],
      "source": [
        "for model in model2lang:\n",
        "    # Load model related to that lang\n",
        "    embedding_model = load_model(model)\n",
        "\n",
        "    for lang in model2lang[model]:\n",
        "        label_columns=lang2labelcolumns.get(lang,lang2labelcolumns[\"others\"])\n",
        "        # Read CSV of that lang\n",
        "        train_data,x_train,y_train=read_csv(tasks=[\"track_a\",\"track_c\"],kinds=[\"train\",\"dev\"],langs=[lang])\n",
        "\n",
        "        # Get class weights tensor\n",
        "        class_weights_tensor = get_class_weights_tensor(y_train)\n",
        "\n",
        "\n",
        "        # Extract embeddings\n",
        "        train_embeddings = extract_embeddings(x_train, embedding_model, batch_size=32)\n",
        "\n",
        "        # Flatten labels for XGBoost (multi-label classification)\n",
        "        y_train_flat = np.array(y_train)\n",
        "        langpath=lang\n",
        "\n",
        "        if os.path.isdir(f\"{local_dir}/models/{langpath}\")==False:\n",
        "            langpath=\"all\"\n",
        "            allLangs.append(lang)\n",
        "            continue\n",
        "            xgb_models = {}\n",
        "\n",
        "            for i, label in enumerate(label_columns):\n",
        "                print(f\"Training XGBoost for label: {label}\")\n",
        "                # Use the computed class weights for the positive class\n",
        "                scale_pos_weight = class_weights_tensor[i].item()\n",
        "                # Initialize and train XGBoost classifier\n",
        "                xgb_model = xgb.XGBClassifier(\n",
        "                    objective='binary:logistic',\n",
        "                    eval_metric='logloss',\n",
        "                    use_label_encoder=False,\n",
        "                    n_estimators=100,\n",
        "                    learning_rate=0.1,\n",
        "                    max_depth=6,\n",
        "                    tree_method='hist',\n",
        "                    scale_pos_weight=scale_pos_weight,  # Apply the class weight here\n",
        "                    device=device\n",
        "                )\n",
        "                xgb_model.fit(train_embeddings, y_train_flat[:, i])\n",
        "                xgb_models[label] = xgb_model\n",
        "            models=xgb_models\n",
        "            save_models(models,f\"{local_dir}/models/{langpath}\",isxgb=True)\n",
        "        else:\n",
        "            svm_models = {}\n",
        "\n",
        "            for i, label in enumerate(label_columns):\n",
        "                print(f\"Training SVM for label: {label}\")\n",
        "\n",
        "                # Use the computed class weights for balancing the classes\n",
        "                class_weight = {0: 1, 1: class_weights_tensor[i].item()}  # Adjust the class weights\n",
        "\n",
        "                svm_model = SVC(class_weight={0: 1, 1: class_weights_tensor[i].item()})\n",
        "\n",
        "                svm_model.fit(train_embeddings, y_train_flat[:, i])\n",
        "                svm_models[label] = svm_model\n",
        "            models=svm_models\n",
        "            save_models(models,f\"{local_dir}/models/{langpath}\",isxgb=False)\n",
        "        # if os.path.exists(f\"{local_dir}/models/{langpath}/svm_models_pkl.zip\"):\n",
        "        #     unzip_models(local_dir, langpath,\"svm_models_pkl.zip\")\n",
        "        # elif os.path.exists(f\"{local_dir}/models/{langpath}/xgb_models_pkl.zip\"):\n",
        "        #     unzip_models(local_dir, langpath,\"xgb_models_pkl.zip\")\n",
        "        # else:\n",
        "        #     print(f\"No models found for task:{task} lang:{lang}\")\n",
        "        #     continue\n",
        "\n",
        "        # models=load_models(f\"{local_dir}/models/{langpath}\")\n",
        "\n",
        "        # Predict labels for the test data\n",
        "        for task in [\"track_a\",\"track_c\"]:\n",
        "            test_data,x_test,y_test=read_csv(tasks=[task],kinds=[\"test\"],langs=[lang])\n",
        "            if test_data is None:\n",
        "                continue\n",
        "            test_embeddings = extract_embeddings(x_test, embedding_model, batch_size=32)\n",
        "            y_test_flat = np.array(y_test)\n",
        "            test_predictions = {}\n",
        "            for label in label_columns:\n",
        "                print(f\"Predicting for label: {label}\")\n",
        "                test_predictions[label] = models[label].predict(test_embeddings)\n",
        "\n",
        "            predictions_df = pd.DataFrame(test_predictions)\n",
        "            test_data.reset_index(inplace=True)\n",
        "\n",
        "            output_df = pd.concat([test_data[['id','lang']], predictions_df], axis=1)\n",
        "\n",
        "            output_dir = task\n",
        "            os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "            # Iterate through the unique languages in the 'lang' column\n",
        "            for lang in output_df['lang'].unique():\n",
        "                # Filter rows for the current language\n",
        "                lang_df = output_df[output_df['lang'] == lang]\n",
        "                lang_df.drop(columns=['lang'],inplace=True)\n",
        "\n",
        "                # Define the output file path for the current language\n",
        "                lang_csv_file = os.path.join(output_dir, f\"pred_{lang}.csv\")\n",
        "\n",
        "                # Sort the DataFrame by the 'id' column\n",
        "                lang_df.sort_values(by='id', inplace=True)\n",
        "\n",
        "                # Save the filtered DataFrame to a CSV file\n",
        "                lang_df.to_csv(lang_csv_file, index=False)\n",
        "\n",
        "                # Print a confirmation message\n",
        "                print(f\"Predictions for language '{lang}' saved to {lang_csv_file}\")\n",
        "    del embedding_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AydYZ3zjRQom"
      },
      "outputs": [],
      "source": [
        "# Load model related to that lang\n",
        "embedding_model = load_model(allModel)\n",
        "\n",
        "label_columns=lang2labelcolumns[\"others\"]\n",
        "# Read CSV of that lang\n",
        "train_data,x_train,y_train=read_csv(tasks=[\"track_a\",\"track_c\"],kinds=[\"train\",\"dev\"],langs=allLangs)\n",
        "\n",
        "# Get class weights tensor\n",
        "class_weights_tensor = get_class_weights_tensor(y_train)\n",
        "\n",
        "# Extract embeddings\n",
        "train_embeddings = extract_embeddings(x_train, embedding_model, batch_size=32)\n",
        "\n",
        "# Flatten labels for XGBoost (multi-label classification)\n",
        "y_train_flat = np.array(y_train)\n",
        "langpath=allLangs[0]\n",
        "\n",
        "if os.path.isdir(f\"{local_dir}/models/{langpath}\")==False:\n",
        "    langpath=\"all\"\n",
        "    xgb_models = {}\n",
        "\n",
        "    for i, label in enumerate(label_columns):\n",
        "        print(f\"Training XGBoost for label: {label}\")\n",
        "        # Use the computed class weights for the positive class\n",
        "        scale_pos_weight = class_weights_tensor[i].item()\n",
        "        # Initialize and train XGBoost classifier\n",
        "        xgb_model = xgb.XGBClassifier(\n",
        "            objective='binary:logistic',\n",
        "            eval_metric='logloss',\n",
        "            use_label_encoder=False,\n",
        "            n_estimators=100,\n",
        "            learning_rate=0.1,\n",
        "            max_depth=6,\n",
        "            tree_method='hist',\n",
        "            scale_pos_weight=scale_pos_weight,  # Apply the class weight here\n",
        "            device=device\n",
        "        )\n",
        "        xgb_model.fit(train_embeddings, y_train_flat[:, i])\n",
        "        xgb_models[label] = xgb_model\n",
        "    models=xgb_models\n",
        "    save_models(models,f\"{local_dir}/models/{langpath}\",isxgb=True)\n",
        "else:\n",
        "    svm_models = {}\n",
        "\n",
        "    for i, label in enumerate(label_columns):\n",
        "        print(f\"Training SVM for label: {label}\")\n",
        "\n",
        "        # Use the computed class weights for balancing the classes\n",
        "        class_weight = {0: 1, 1: class_weights_tensor[i].item()}  # Adjust the class weights\n",
        "\n",
        "        svm_model = SVC(class_weight={0: 1, 1: class_weights_tensor[i].item()})\n",
        "\n",
        "        svm_model.fit(train_embeddings, y_train_flat[:, i])\n",
        "        svm_models[label] = svm_model\n",
        "    models=svm_models\n",
        "    save_models(models,f\"{local_dir}/models/{langpath}\",isxgb=False)\n",
        "# if os.path.exists(f\"{local_dir}/models/{langpath}/svm_models_pkl.zip\"):\n",
        "#     unzip_models(local_dir, langpath,\"svm_models_pkl.zip\")\n",
        "# elif os.path.exists(f\"{local_dir}/models/{langpath}/xgb_models_pkl.zip\"):\n",
        "#     unzip_models(local_dir, langpath,\"xgb_models_pkl.zip\")\n",
        "# else:\n",
        "#     print(f\"No models found for task:{task} lang:{lang}\")\n",
        "#     continue\n",
        "\n",
        "# models=load_models(f\"{local_dir}/models/{langpath}\")\n",
        "\n",
        "# Predict labels for the test data\n",
        "for lang in allLangs:\n",
        "    for task in [\"track_a\",\"track_c\"]:\n",
        "        test_data,x_test,y_test=read_csv(tasks=[task],kinds=[\"test\"],langs=[lang])\n",
        "        if test_data is None:\n",
        "            continue\n",
        "        test_embeddings = extract_embeddings(x_test, embedding_model, batch_size=32)\n",
        "        y_test_flat = np.array(y_test)\n",
        "        test_predictions = {}\n",
        "        for label in label_columns:\n",
        "            print(f\"Predicting for label: {label}\")\n",
        "            test_predictions[label] = models[label].predict(test_embeddings)\n",
        "\n",
        "        predictions_df = pd.DataFrame(test_predictions)\n",
        "        test_data.reset_index(inplace=True)\n",
        "\n",
        "        output_df = pd.concat([test_data[['id','lang']], predictions_df], axis=1)\n",
        "\n",
        "        output_dir = task\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "        # Iterate through the unique languages in the 'lang' column\n",
        "        for lang in output_df['lang'].unique():\n",
        "            # Filter rows for the current language\n",
        "            lang_df = output_df[output_df['lang'] == lang]\n",
        "            lang_df.drop(columns=['lang'],inplace=True)\n",
        "\n",
        "            # Define the output file path for the current language\n",
        "            lang_csv_file = os.path.join(output_dir, f\"pred_{lang}.csv\")\n",
        "\n",
        "            # Sort the DataFrame by the 'id' column\n",
        "            lang_df.sort_values(by='id', inplace=True)\n",
        "\n",
        "            # Save the filtered DataFrame to a CSV file\n",
        "            lang_df.to_csv(lang_csv_file, index=False)\n",
        "\n",
        "            # Print a confirmation message\n",
        "            print(f\"Predictions for language '{lang}' saved to {lang_csv_file}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t7196oGQFK--"
      },
      "outputs": [],
      "source": [
        "task=\"track_a\"\n",
        "!zip -r {task}.zip {task}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eJ89U0khMXAq"
      },
      "outputs": [],
      "source": [
        "task=\"track_c\"\n",
        "!zip -r {task}.zip {task}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7hlDcHyq2mA4"
      },
      "outputs": [],
      "source": [
        "!git lfs install"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IkfP4bqW2mA4"
      },
      "outputs": [],
      "source": [
        "!cd {local_dir}\n",
        "!git config --global user.email \"sadeghpoolaee@gmail.com\"\n",
        "!git config --global user.name \"mspoulaei\"\n",
        "!git remote set-url origin https://huggingface.co/ErfanSadegh/SemEval2025-Task11-TrackA\n",
        "!git add .\n",
        "!git commit -m \"Version 2\"\n",
        "!git push"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [],
      "dockerImageVersionId": 30823,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
