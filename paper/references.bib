% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").
@article{hu2021lora,
  title   = {LoRA: Low-Rank Adaptation of Large Language Models},
  author  = {Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Chen, Weizhu},
  journal = {arXiv preprint arXiv:2106.09685},
  year    = {2021}
}

@inproceedings{chen2016xgboost,
  title        = {XGBoost: A Scalable Tree Boosting System},
  author       = {Chen, Tianqi and Guestrin, Carlos},
  booktitle    = {Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
  pages        = {785--794},
  year         = {2016},
  organization = {ACM}
}

@article{cortes1995support,
  title     = {Support-Vector Networks},
  author    = {Cortes, Corinna and Vapnik, Vladimir},
  journal   = {Machine Learning},
  volume    = {20},
  number    = {3},
  pages     = {273--297},
  year      = {1995},
  publisher = {Springer}
}
@book{Goodfellow-et-al-2016,
  title     = {Deep Learning},
  author    = {Ian Goodfellow and Yoshua Bengio and Aaron Courville},
  year      = {2016},
  publisher = {MIT Press}
}

@article{Zhang2020DataPrep,
  title   = {A Comprehensive Guide to Text Data Preprocessing},
  author  = {Zhang, Li and Wang, Jun},
  journal = {Journal of Data Science},
  volume  = {12},
  year    = {2020},
  pages   = {45--67}
}

@article{Akiba2019Optuna,
  title   = {Optuna: A Next-generation Hyperparameter Optimization Framework},
  author  = {Akiba, Takuya and Sano, Shotaro and Yanase, Toshihiko and Ohta, Takeru and Koyama, Masanori},
  journal = {Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
  year    = {2019},
  pages   = {2623--2631}
}

@article{Loshchilov2019AdamW,
  title   = {Decoupled Weight Decay Regularization},
  author  = {Loshchilov, Ilya and Hutter, Frank},
  journal = {International Conference on Learning Representations},
  year    = {2019}
}

@article{Paszke2019PyTorch,
  title   = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
  author  = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and others},
  journal = {Advances in Neural Information Processing Systems},
  volume  = {32},
  year    = {2019}
}

@article{Pedregosa2011ScikitLearn,
  title   = {Scikit-learn: Machine Learning in Python},
  author  = {Pedregosa, Fabian and Varoquaux, Ga{\"e}l and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and others},
  journal = {Journal of Machine Learning Research},
  volume  = {12},
  year    = {2011},
  pages   = {2825--2830}
}


@article{Wolf2019HuggingFace,
  title   = {Transformers: State-of-the-Art Natural Language Processing},
  author  = {Wolf, Thomas and Debut, Lysandre and Sanh, Victor and Chaumond, Julien and Delangue, Clement and Moi, Anthony and Cistac, Pierric and Rault, Tim and Louf, Remi and Funtowicz, Morgan and others},
  journal = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations},
  pages   = {38--45},
  year    = {2020}
}

@inproceedings{wang2016dimensional,
  title     = {Dimensional sentiment analysis using a regional CNN-LSTM model},
  author    = {Wang, Jin and Yu, Liang-Chih and Lai, K Robert and Zhang, Xuejie},
  booktitle = {Proceedings of the 54th annual meeting of the association for computational linguistics (volume 2: Short papers)},
  pages     = {225--230},
  year      = {2016}
}

@article{muhammad2023semeval,
  title   = {SemEval-2023 task 12: sentiment analysis for african languages (AfriSenti-SemEval)},
  author  = {Muhammad, Shamsuddeen Hassan and Abdulmumin, Idris and Yimam, Seid Muhie and Adelani, David Ifeoluwa and Ahmad, Ibrahim Sa'id and Ousidhoum, Nedjma and Ayele, Abinew and Mohammad, Saif M and Beloucif, Meriem and Ruder, Sebastian},
  journal = {arXiv preprint arXiv:2304.06845},
  year    = {2023}
}

@misc{muhammad2025brighterbridginggaphumanannotated,
  title         = {BRIGHTER: BRIdging the Gap in Human-Annotated Textual Emotion Recognition Datasets for 28 Languages},
  author        = {Shamsuddeen Hassan Muhammad and Nedjma Ousidhoum and Idris Abdulmumin and Jan Philip Wahle and Terry Ruas and Meriem Beloucif and Christine de Kock and Nirmal Surange and Daniela Teodorescu and Ibrahim Said Ahmad and David Ifeoluwa Adelani and 
                   Alham Fikri Aji and Felermino D. M. A. Ali and Ilseyar Alimova and Vladimir Araujo and Nikolay Babakov and Naomi Baes and Ana-Maria Bucur and Andiswa Bukula and Guanqun Cao and Rodrigo Tufino Cardenas and Rendi Chevi and Chiamaka Ijeoma Chukwuneke and 
                   Alexandra Ciobotaru and Daryna Dementieva and Murja Sani Gadanya and Robert Geislinger and Bela Gipp and Oumaima Hourrane and Oana Ignat and Falalu Ibrahim Lawan and Rooweither Mabuya and Rahmad Mahendra and Vukosi Marivate and Andrew Piper and Alexander 
                   Panchenko and Charles Henrique Porto Ferreira and Vitaly Protasov and Samuel Rutunda and Manish Shrivastava and Aura Cristina Udrea and Lilian Diana Awuor Wanzare and Sophie Wu and Florian Valentin Wunderlich and Hanif Muhammad Zhafran and Tianhui Zhang 
                   and Yi Zhou and Saif M. Mohammad},
  year          = {2025},
  eprint        = {2502.11926},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/2502.11926}
}

@inproceedings{belay-etal-2025-evaluating,
  title     = {Evaluating the Capabilities of Large Language Models for Multi-label Emotion Understanding},
  author    = {Belay, Tadesse Destaw  and Azime, Israel Abebe  and Ayele, Abinew Ali  and Sidorov, Grigori  and Klakow, Dietrich  and Slusallek, Philip  and Kolesnikova, Olga  and Yimam, Seid Muhie},
  booktitle = {Proceedings of the 31st International Conference on Computational Linguistics},
  year      = {2025},
  address   = {Abu Dhabi, UAE},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2025.coling-main.237/},
  pages     = {3523--3540}
}

@article{wang2024multilingual,
  title   = {Multilingual E5 Text Embeddings: A Technical Report},
  author  = {Wang, Liang and Yang, Nan and Huang, Xiaolong and Yang, Linjun and Majumder, Rangan and Wei, Furu},
  journal = {arXiv preprint arXiv:2402.05672},
  year    = {2024}
}
@article{arabert,
  title     = {Sentiment Analysis on Algerian Dialect with Transformers},
  author    = {Zakaria Benmounah and Abdennour Boulesnane and Abdeladim Fadheli and Mustapha Khial},
  journal   = {Applied Sciences},
  volume    = {13},
  number    = {20},
  pages     = {11157},
  year      = {2023},
  month     = {Oct},
  publisher = {MDPI AG},
  doi       = {10.3390/app132011157},
  issn      = {2076-3417},
  url       = {http://dx.doi.org/10.3390/app132011157}
}
@article{muennighoff2022mteb,
  doi       = {10.48550/ARXIV.2210.07316},
  url       = {https://arxiv.org/abs/2210.07316},
  author    = {Muennighoff, Niklas and Tazi, Nouamane and Magne, Lo{\"\i}c and Reimers, Nils},
  title     = {MTEB: Massive Text Embedding Benchmark},
  publisher = {arXiv},
  journal   = {arXiv preprint arXiv:2210.07316},
  year      = {2022}
}

@misc{zhang2025jasperstelladistillationsota,
  title         = {Jasper and Stella: distillation of SOTA embedding models},
  author        = {Dun Zhang and Jiacheng Li and Ziyang Zeng and Fulong Wang},
  year          = {2025},
  eprint        = {2412.19048},
  archiveprefix = {arXiv},
  primaryclass  = {cs.IR},
  url           = {https://arxiv.org/abs/2412.19048}
}
@misc{snegirev2025russianfocusedembeddersexplorationrumteb,
  title         = {The Russian-focused embedders' exploration: ruMTEB benchmark and Russian embedding model design},
  author        = {Artem Snegirev and Maria Tikhonova and Anna Maksimova and Alena Fenogenova and Alexander Abramov},
  year          = {2025},
  eprint        = {2408.12503},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/2408.12503}
}

@misc{sturua2024jinaembeddingsv3multilingualembeddingstask,
  title         = {jina-embeddings-v3: Multilingual Embeddings With Task LoRA},
  author        = {Saba Sturua and Isabelle Mohr and Mohammad Kalim Akram and Michael Günther and Bo Wang and Markus Krimmel and Feng Wang and Georgios Mastrapas and Andreas Koukounas and Andreas Koukounas and Nan Wang and Han Xiao},
  year          = {2024},
  eprint        = {2409.10173},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/2409.10173}
}
@misc{dobler-demelo-2023-focus,
  title         = {FOCUS: Effective Embedding Initialization for Monolingual Specialization of Multilingual Models},
  author        = {Konstantin Dobler and Gerard de Melo},
  year          = {2023},
  eprint        = {2305.14481},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@misc{loshchilov2017sgdrstochasticgradientdescent,
  title         = {SGDR: Stochastic Gradient Descent with Warm Restarts},
  author        = {Ilya Loshchilov and Frank Hutter},
  year          = {2017},
  eprint        = {1608.03983},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG},
  url           = {https://arxiv.org/abs/1608.03983}
}
@inproceedings{reimers-2019-sentence-bert,
  title     = {Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks},
  author    = {Reimers, Nils and Gurevych, Iryna},
  booktitle = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing},
  month     = {11},
  year      = {2019},
  publisher = {Association for Computational Linguistics},
  url       = {https://arxiv.org/abs/1908.10084}
}
@inproceedings{reimers-2020-multilingual-sentence-bert,
  title     = {Making Monolingual Sentence Embeddings Multilingual using Knowledge Distillation},
  author    = {Reimers, Nils and Gurevych, Iryna},
  booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing},
  month     = {11},
  year      = {2020},
  publisher = {Association for Computational Linguistics},
  url       = {https://arxiv.org/abs/2004.09813}
}
@misc{zpoint-large-embedding-zh,
  author       = {iampanda},
  title        = {zpoint-large-embedding-zh},
  year         = {2024},
  month        = {Jun},
  day          = {4},
  howpublished = {\url{https://huggingface.co/iampanda/zpoint_large_embedding_zh}}
}

%appendix:
@misc{feng2022languageagnosticbertsentenceembedding,
      title={Language-agnostic BERT Sentence Embedding}, 
      author={Fangxiaoyu Feng and Yinfei Yang and Daniel Cer and Naveen Arivazhagan and Wei Wang},
      year={2022},
      eprint={2007.01852},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2007.01852}, 
}
@misc{all-MiniLM-L12-v2,
  author = {sentence-transformers},
  title = {all-MiniLM-L12-v2},
  year = {2024},
  month = {Jun},
  day = {4},
  howpublished = {\url{https://huggingface.co/sentence-transformers/all-MiniLM-L12-v2}},
}
@article{lee2024nv,
  title={NV-Embed: Improved Techniques for Training LLMs as Generalist Embedding Models},
  author={Lee, Chankyu and Roy, Rajarshi and Xu, Mengyao and Raiman, Jonathan and Shoeybi, Mohammad and Catanzaro, Bryan and Ping, Wei},
  journal={arXiv preprint arXiv:2405.17428},
  year={2024}
}
@misc{rasyosef2025bertamharic,
  author       = {Yosef, Rasy},
  title        = {BERT Amharic Text Embedding (Medium)},
  year         = {2025},
  howpublished = {\url{https://huggingface.co/rasyosef/bert-amharic-text-embedding-medium}},
  note         = {Accessed: 2025-02-24}
}
@misc{davlan2025bertamharic,
  author       = {Davlan},
  title        = {BERT Base Multilingual Cased Fine-tuned on Amharic},
  year         = {2025},
  howpublished = {\url{https://huggingface.co/Davlan/bert-base-multilingual-cased-finetuned-amharic}},
  note         = {Accessed: 2025-02-24}
}
@misc{rasyosef2025llamaamharic,
  author       = {Rasyosef},
  title        = {Llama 3.2 1B Fine-tuned on Amharic},
  year         = {2025},
  howpublished = {\url{https://huggingface.co/rasyosef/Llama-3.2-1B-Amharic}},
  note         = {Accessed: 2025-02-24}
}
@misc{rasyosef2025robertaamharic,
  author       = {Rasyosef},
  title        = {RoBERTa Amharic Text Embedding (Medium)},
  year         = {2025},
  howpublished = {\url{https://huggingface.co/rasyosef/roberta-amharic-text-embedding-medium}},
  note         = {Accessed: 2025-02-24}
}
@article{dziribert,
  title={DziriBERT: a Pre-trained Language Model for the Algerian Dialect},
  author={Abdaoui, Amine and Berrimi, Mohamed and Oussalah, Mourad and Moussaoui, Abdelouahab},
  journal={arXiv preprint arXiv:2109.12346},
  year={2021}
}

@misc{nacar2025GATE,
      title={GATE: General Arabic Text Embedding for Enhanced Semantic Textual Similarity with Hybrid Loss Training}, 
      author={Omer Nacar, Anis Koubaa, Serry Taiseer Sibaee and Lahouari Ghouti},
      year={2025},
      note={Submitted to COLING 2025},
      url={https://huggingface.co/Omartificial-Intelligence-Space/GATE-AraBert-v1},
}
@inproceedings{safaya-etal-2020-kuisail,
    title = "{KUISAIL} at {S}em{E}val-2020 Task 12: {BERT}-{CNN} for Offensive Speech Identification in Social Media",
    author = "Safaya, Ali  and
      Abdullatif, Moutasem  and
      Yuret, Deniz",
    booktitle = "Proceedings of the Fourteenth Workshop on Semantic Evaluation",
    month = dec,
    year = "2020",
    address = "Barcelona (online)",
    publisher = "International Committee for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.semeval-1.271",
    pages = "2054--2059",
}
@article{gaanoun2023darijabert,
  title={Darijabert: a Step Forward in Nlp for the Written Moroccan Dialect},
  author={Gaanoun, Kamel and Naira, Abdou Mohamed and Allak, Anass and Benelallam, Imade},
  year={2023}
}

@misc{li2024conanembeddinggeneraltextembedding,
  title={Conan-embedding: General Text Embedding with More and Better Negative Samples}, 
  author={Shiyu Li and Yang Tang and Shizhe Chen and Xi Chen},
  year={2024},
  eprint={2408.15710},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url={https://arxiv.org/abs/2408.15710}, 
}
@misc{zhang2024gme,
      title={GME: Improving Universal Multimodal Retrieval by Multimodal LLMs}, 
      author={Zhang, Xin and Zhang, Yanzhao and Xie, Wen and Li, Mingxin and Dai, Ziqi and Long, Dingkun and Xie, Pengjun and Zhang, Meishan and Li, Wenjie and Zhang, Min},
      year={2024},
      eprint={2412.16855},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={http://arxiv.org/abs/2412.16855}, 
}
@misc{lier0072023xiaobuembeddingv2,
  author       = {lier007},
  title        = {xiaobu-embedding-v2: A Text Embedding Model},
  year         = {2023},
  howpublished = {Hugging Face Model Hub},
  url          = {https://huggingface.co/lier007/xiaobu-embedding-v2},
  note         = {Accessed: 2025-02-25}
}
@misc{heinz2023e5basestsende,
  author       = {Daniel Heinz},
  title        = {e5-base-sts-en-de: A Bilingual Text Embedding Model for English and German},
  year         = {2023},
  howpublished = {Hugging Face Model Hub},
  url          = {https://huggingface.co/danielheinz/e5-base-sts-en-de},
  note         = {Accessed: 2025-02-25}
}
@misc{chan2020germanslanguagemodel,
      title={German's Next Language Model}, 
      author={Branden Chan and Stefan Schweter and Timo Möller},
      year={2020},
      eprint={2010.10906},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2010.10906}, 
}
@misc{chibb2023germansemanticsts,
  author       = {Aaron Chibb},
  title        = {German\_Semantic\_STS\_V2},
  year         = 2023,
  publisher    = {Hugging Face},
  journal      = {Hugging Face Repository},
  howpublished = {\url{https://huggingface.co/aari1995/German_Semantic_STS_V2}},
}
@article{mohr2024multi,
  title={Multi-Task Contrastive Learning for 8192-Token Bilingual Text Embeddings},
  author={Mohr, Isabelle and Krimmel, Markus and Sturua, Saba and Akram, Mohammad Kalim and Koukounas, Andreas and G{\"u}nther, Michael and Mastrapas, Georgios and Ravishankar, Vinit and Mart{\'\i}nez, Joan Fontanals and Wang, Feng and others},
  journal={arXiv preprint arXiv:2402.17016},
  year={2024}
}
@misc{ni2021sentencet5scalablesentenceencoders,
      title={Sentence-T5: Scalable Sentence Encoders from Pre-trained Text-to-Text Models}, 
      author={Jianmo Ni and Gustavo Hernández Ábrego and Noah Constant and Ji Ma and Keith B. Hall and Daniel Cer and Yinfei Yang},
      year={2021},
      eprint={2108.08877},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2108.08877}, 
}
@article{wang2023improving,
  title={Improving Text Embeddings with Large Language Models},
  author={Wang, Liang and Yang, Nan and Huang, Xiaolong and Yang, Linjun and Majumder, Rangan and Wei, Furu},
  journal={arXiv preprint arXiv:2401.00368},
  year={2023}
}

@article{DBLP:journals/corr/abs-1810-04805,
  author    = {Jacob Devlin and
               Ming{-}Wei Chang and
               Kenton Lee and
               Kristina Toutanova},
  title     = {{BERT:} Pre-training of Deep Bidirectional Transformers for Language
               Understanding},
  journal   = {CoRR},
  volume    = {abs/1810.04805},
  year      = {2018},
  url       = {http://arxiv.org/abs/1810.04805},
  archivePrefix = {arXiv},
  eprint    = {1810.04805},
  timestamp = {Tue, 30 Oct 2018 20:39:56 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1810-04805.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{DBLP:journals/corr/abs-1907-11692,
  author    = {Yinhan Liu and
               Myle Ott and
               Naman Goyal and
               Jingfei Du and
               Mandar Joshi and
               Danqi Chen and
               Omer Levy and
               Mike Lewis and
               Luke Zettlemoyer and
               Veselin Stoyanov},
  title     = {RoBERTa: {A} Robustly Optimized {BERT} Pretraining Approach},
  journal   = {CoRR},
  volume    = {abs/1907.11692},
  year      = {2019},
  url       = {http://arxiv.org/abs/1907.11692},
  archivePrefix = {arXiv},
  eprint    = {1907.11692},
  timestamp = {Thu, 01 Aug 2019 08:59:33 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1907-11692.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{DBLP:journals/corr/abs-1911-02116,
  author    = {Alexis Conneau and
               Kartikay Khandelwal and
               Naman Goyal and
               Vishrav Chaudhary and
               Guillaume Wenzek and
               Francisco Guzm{\'{a}}n and
               Edouard Grave and
               Myle Ott and
               Luke Zettlemoyer and
               Veselin Stoyanov},
  title     = {Unsupervised Cross-lingual Representation Learning at Scale},
  journal   = {CoRR},
  volume    = {abs/1911.02116},
  year      = {2019},
  url       = {http://arxiv.org/abs/1911.02116},
  eprinttype = {arXiv},
  eprint    = {1911.02116},
  timestamp = {Mon, 11 Nov 2019 18:38:09 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1911-02116.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{CaneteCFP2020,
  title={Spanish Pre-Trained BERT Model and Evaluation Data},
  author={Cañete, José and Chaperon, Gabriel and Fuentes, Rodrigo and Ho, Jou-Hui and Kang, Hojin and Pérez, Jorge},
  booktitle={PML4DC at ICLR 2020},
  year={2020}
}
@misc{romero2023multilinguale5largeftstsspanish,
  author       = {Manuel Romero},
  title        = {multilingual-e5-large-ft-sts-spanish-matryoshka-768-64-5e},
  year         = 2023,
  publisher    = {Hugging Face},
  journal      = {Hugging Face Repository},
  howpublished = {\url{https://huggingface.co/mrm8488/multilingual-e5-large-ft-sts-spanish-matryoshka-768-64-5e}},
}
@misc{oketunji2024pmmlv2finetunedhausa,
  author       = {Finbarrs Oketunji},
  title        = {pmmlv2-fine-tuned-hausa},
  year         = 2024,
  publisher    = {Hugging Face},
  journal      = {Hugging Face Repository},
  doi          = {10.57967/hf/2959},
  howpublished = {\url{https://huggingface.co/0xnu/pmmlv2-fine-tuned-hausa}},
}
@misc{sukhlecha_2024_bhasha_embed_v0,
  author = {Sukhlecha, Akshita},
  title = {Bhasha-embed-v0},
  howpublished = {Hugging Face},
  month = {June},
  year = {2024},
  url = {https://huggingface.co/AkshitaS/bhasha-embed-v0}
}
@article{joshi2022l3cubemahasbert,
  title={L3Cube-MahaSBERT and HindSBERT: Sentence BERT Models and Benchmarking BERT Sentence Representations for Hindi and Marathi},
  author={Joshi, Ananya and Kajale, Aditi and Gadre, Janhavi and Deode, Samruddhi and Joshi, Raviraj},
  journal={arXiv preprint arXiv:2211.11187},
  year={2022}
}
@misc{nogueira2019documentexpansionqueryprediction,
      title={Document Expansion by Query Prediction}, 
      author={Rodrigo Nogueira and Wei Yang and Jimmy Lin and Kyunghyun Cho},
      year={2019},
      eprint={1904.08375},
      archivePrefix={arXiv},
      primaryClass={cs.IR},
      url={https://arxiv.org/abs/1904.08375}, 
}
@misc{feng2020languageagnostic,
      title={Language-agnostic BERT Sentence Embedding},
      author={Fangxiaoyu Feng and Yinfei Yang and Daniel Cer and Naveen Arivazhagan and Wei Wang},
      year={2020},
      eprint={2007.01852},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{oketunji2024pmmlv2finetunedigbo,
  author       = {Finbarrs Oketunji},
  title        = {pmmlv2-fine-tuned-igbo},
  year         = 2024,
  publisher    = {Hugging Face},
  journal      = {Hugging Face Repository},
  doi          = {10.57967/hf/2960},
  howpublished = {\url{https://huggingface.co/0xnu/pmmlv2-fine-tuned-igbo}},
}
@misc{adelani2023bertmultilingualkinyarwanda,
  author       = {David Adelani},
  title        = {bert-base-multilingual-cased-finetuned-kinyarwanda},
  year         = 2023,
  publisher    = {Hugging Face},
  journal      = {Hugging Face Repository},
  howpublished = {\url{https://huggingface.co/Davlan/bert-base-multilingual-cased-finetuned-kinyarwanda}},
}
@misc{adelani2023xlmrobertakinyrwanda,
  author       = {David Adelani},
  title        = {xlm-roberta-large-finetuned-kinyarwanda},
  year         = 2023,
  publisher    = {Hugging Face},
  journal      = {Hugging Face Repository},
  howpublished = {\url{https://huggingface.co/Davlan/xlm-roberta-large-finetuned-kinyarwanda}},
}
@misc{filho2023bertportuguesenliassin2,
  author       = {Ricardo Filho},
  title        = {bert-base-portuguese-cased-nli-assin-2},
  year         = 2023,
  publisher    = {Hugging Face},
  journal      = {Hugging Face Repository},
  howpublished = {\url{https://huggingface.co/ricardo-filho/bert-base-portuguese-cased-nli-assin-2}},
}
@inproceedings{souza2020bertimbau,
  author    = {F{\'a}bio Souza and
               Rodrigo Nogueira and
               Roberto Lotufo},
  title     = {{BERT}imbau: pretrained {BERT} models for {B}razilian {P}ortuguese},
  booktitle = {9th Brazilian Conference on Intelligent Systems, {BRACIS}, Rio Grande do Sul, Brazil, October 20-23 (to appear)},
  year      = {2020}
}
@misc{melo2023bertlargeportuguesests,
  author       = {Rui Melo},
  title        = {bert-large-portuguese-cased-sts},
  year         = 2023,
  publisher    = {Hugging Face},
  journal      = {Hugging Face Repository},
  howpublished = {\url{https://huggingface.co/rufimelo/bert-large-portuguese-cased-sts}},
}
@inproceedings{kummervold-etal-2021-operationalizing,
title = {Operationalizing a National Digital Library: The Case for a Norwegian Transformer Model},
author = {Kummervold, Per E  and
  De la Rosa, Javier  and
  Wetjen, Freddy  and
  Brygfjeld, Svein Arne},
booktitle = {Proceedings of the 23rd Nordic Conference on Computational Linguistics (NoDaLiDa)},
year = {2021},
address = {Reykjavik, Iceland (Online)},
publisher = {Link{\"o}ping University Electronic Press, Sweden},
url = {https://aclanthology.org/2021.nodalida-main.3},
pages = {20--29},
abstract = {In this work, we show the process of building a large-scale training set from digital and digitized collections at a national library. The resulting Bidirectional Encoder Representations from Transformers (BERT)-based language model for Norwegian outperforms multilingual BERT (mBERT) models in several token and sequence classification tasks for both Norwegian Bokm{\aa}l and Norwegian Nynorsk. Our model also improves the mBERT performance for other languages present in the corpus such as English, Swedish, and Danish. For languages not included in the corpus, the weights degrade moderately while keeping strong multilingual properties. Therefore, we show that building high-quality models within a memory institution using somewhat noisy optical character recognition (OCR) content is feasible, and we hope to pave the way for other memory institutions to follow.},
}
@misc{schweter2020electraukrainian,
  author       = {Stefan Schweter},
  title        = {electra-base-ukrainian-cased-discriminator},
  year         = 2020,
  publisher    = {Hugging Face},
  journal      = {Hugging Face Repository},
  howpublished = {\url{https://huggingface.co/lang-uk/electra-base-ukrainian-cased-discriminator}},
}
@inproceedings{laba-etal-2023-contextual,
    title = "Contextual Embeddings for {U}krainian: A Large Language Model Approach to Word Sense Disambiguation",
    author = "Laba, Yurii  and
      Mudryi, Volodymyr  and
      Chaplynskyi, Dmytro  and
      Romanyshyn, Mariana  and
      Dobosevych, Oles",
    editor = "Romanyshyn, Mariana",
    booktitle = "Proceedings of the Second Ukrainian Natural Language Processing Workshop (UNLP)",
    month = may,
    year = "2023",
    address = "Dubrovnik, Croatia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.unlp-1.2",
    doi = "10.18653/v1/2023.unlp-1.2",
    pages = "11--19",
    abstract = "This research proposes a novel approach to the Word Sense Disambiguation (WSD) task in the Ukrainian language based on supervised fine-tuning of a pre-trained Large Language Model (LLM) on the dataset generated in an unsupervised way to obtain better contextual embeddings for words with multiple senses. The paper presents a method for generating a new dataset for WSD evaluation in the Ukrainian language based on the SUM dictionary. We developed a comprehensive framework that facilitates the generation of WSD evaluation datasets, enables the use of different prediction strategies, LLMs, and pooling strategies, and generates multiple performance reports. Our approach shows 77,9{\%} accuracy for lexical meaning prediction for homonyms.",
}
@misc{minixhofer2023robertaukraine,
  author       = {Benjamin Minixhofer},
  title        = {roberta-large-wechsel-ukrainian},
  year         = 2023,
  publisher    = {Hugging Face},
  journal      = {Hugging Face Repository},
  howpublished = {\url{https://huggingface.co/benjamin/roberta-large-wechsel-ukrainian}},
}
@misc{reimers2020makingmonolingualsentenceembeddings,
      title={Making Monolingual Sentence Embeddings Multilingual using Knowledge Distillation}, 
      author={Nils Reimers and Iryna Gurevych},
      year={2020},
      eprint={2004.09813},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2004.09813}, 
}
@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}
@misc{llama3.2-1b-instruct,
  title = {Llama 3.2 1B Instruct Model},
  author = {Meta AI},
  year = {2025},
  howpublished = {\url{https://github.com/meta/llama}},
  note = {A fine-tuned version of the Llama 3.2 model optimized for instruction-following tasks.}
}
@misc{abdin2024phi3technicalreporthighly,
      title={Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone}, 
      author={Marah Abdin and Jyoti Aneja and Hany Awadalla and Ahmed Awadallah and Ammar Ahmad Awan and Nguyen Bach and Amit Bahree and Arash Bakhtiari and Jianmin Bao and Harkirat Behl and Alon Benhaim and Misha Bilenko and Johan Bjorck and Sébastien Bubeck and Martin Cai and Qin Cai and Vishrav Chaudhary and Dong Chen and Dongdong Chen and Weizhu Chen and Yen-Chun Chen and Yi-Ling Chen and Hao Cheng and Parul Chopra and Xiyang Dai and Matthew Dixon and Ronen Eldan and Victor Fragoso and Jianfeng Gao and Mei Gao and Min Gao and Amit Garg and Allie Del Giorno and Abhishek Goswami and Suriya Gunasekar and Emman Haider and Junheng Hao and Russell J. Hewett and Wenxiang Hu and Jamie Huynh and Dan Iter and Sam Ade Jacobs and Mojan Javaheripi and Xin Jin and Nikos Karampatziakis and Piero Kauffmann and Mahoud Khademi and Dongwoo Kim and Young Jin Kim and Lev Kurilenko and James R. Lee and Yin Tat Lee and Yuanzhi Li and Yunsheng Li and Chen Liang and Lars Liden and Xihui Lin and Zeqi Lin and Ce Liu and Liyuan Liu and Mengchen Liu and Weishung Liu and Xiaodong Liu and Chong Luo and Piyush Madan and Ali Mahmoudzadeh and David Majercak and Matt Mazzola and Caio César Teodoro Mendes and Arindam Mitra and Hardik Modi and Anh Nguyen and Brandon Norick and Barun Patra and Daniel Perez-Becker and Thomas Portet and Reid Pryzant and Heyang Qin and Marko Radmilac and Liliang Ren and Gustavo de Rosa and Corby Rosset and Sambudha Roy and Olatunji Ruwase and Olli Saarikivi and Amin Saied and Adil Salim and Michael Santacroce and Shital Shah and Ning Shang and Hiteshi Sharma and Yelong Shen and Swadheen Shukla and Xia Song and Masahiro Tanaka and Andrea Tupini and Praneetha Vaddamanu and Chunyu Wang and Guanhua Wang and Lijuan Wang and Shuohang Wang and Xin Wang and Yu Wang and Rachel Ward and Wen Wen and Philipp Witte and Haiping Wu and Xiaoxia Wu and Michael Wyatt and Bin Xiao and Can Xu and Jiahang Xu and Weijian Xu and Jilong Xue and Sonali Yadav and Fan Yang and Jianwei Yang and Yifan Yang and Ziyi Yang and Donghan Yu and Lu Yuan and Chenruidong Zhang and Cyril Zhang and Jianwen Zhang and Li Lyna Zhang and Yi Zhang and Yue Zhang and Yunan Zhang and Xiren Zhou},
      year={2024},
      eprint={2404.14219},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2404.14219}, 
}
@misc{qwen2.5,
    title = {Qwen2.5: A Party of Foundation Models},
    url = {https://qwenlm.github.io/blog/qwen2.5/},
    author = {Qwen Team},
    month = {September},
    year = {2024}
}
@article{gemma_2024,
    title={Gemma},
    url={https://www.kaggle.com/m/3301},
    DOI={10.34740/KAGGLE/M/3301},
    publisher={Kaggle},
    author={Gemma Team},
    year={2024}
}
@article{DBLP:journals/corr/abs-1910-13461,
  author    = {Mike Lewis and
               Yinhan Liu and
               Naman Goyal and
               Marjan Ghazvininejad and
               Abdelrahman Mohamed and
               Omer Levy and
               Veselin Stoyanov and
               Luke Zettlemoyer},
  title     = {{BART:} Denoising Sequence-to-Sequence Pre-training for Natural Language
               Generation, Translation, and Comprehension},
  journal   = {CoRR},
  volume    = {abs/1910.13461},
  year      = {2019},
  url       = {http://arxiv.org/abs/1910.13461},
  eprinttype = {arXiv},
  eprint    = {1910.13461},
  timestamp = {Thu, 31 Oct 2019 14:02:26 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1910-13461.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@misc{dai2020funneltransformer,
    title={Funnel-Transformer: Filtering out Sequential Redundancy for Efficient Language Processing},
    author={Zihang Dai and Guokun Lai and Yiming Yang and Quoc V. Le},
    year={2020},
    eprint={2006.03236},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}
@inproceedings{clark2020electra,
  title={ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators},
  author={Clark, Kevin and Luong, Minh-Thang and Le, Quoc V and Manning, Christopher D},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2020},
  url={https://openreview.net/forum?id=r1xMH1BtvB}
}
