\section{Experimental Setup}

This section describes the experimental setup, providing details about data splits, preprocessing steps, hyperparameter tuning, computational resources, and the tools and libraries utilized. The goal is to ensure reproducibility, transparency, and alignment with established best practices in machine learning research.
% \input{tables/MainResults}
\begin{table*}[h]
    \centering
    \caption{Results across Track A, B, and C showing macro-average F1 scores of Our Model , Paraticipants Best Model scores, Task Dataset Model(Baseline)\citep{muhammad2025brighterbridginggaphumanannotated} and rankings.}
    \label{tab:results}
    \resizebox{2\columnwidth}{!}{
        \begin{tabular}{l|c|ccc|cccc|cccc}
            \hline
                                                    &                                                                                              & \multicolumn{3}{c|}{Track A} & \multicolumn{4}{c|}{Track B} & \multicolumn{4}{c}{Track C}                                                                                                                                         \\
            \cline{3-13}
            \textbf{Language}                       & \textbf{Our Model}                                                                           & \textbf{Ours}                & \textbf{Best}                & \textbf{Rank}               & \textbf{Ours} & \textbf{Best} & \textbf{Baseline} & \textbf{Rank} & \textbf{Ours} & \textbf{Best} & \textbf{Baseline} & \textbf{Rank} \\
            \hline
            Afrikaans(afr)                          & \citep{wang2024multilingual} + SVM                                                           & 54.01                        & 69.86                        & 15/37                       & —             & —             & —                 & —             & 54.01         & 70.50         & 61.28             & 4/14          \\
            Amharic(amh)                            & \citep{arabert}               + SVM                                                          & 61.20                        & 77.31                        & 19/43                       & 49.42         & 85.58         & —                 & 14/23         & 61.20         & 66.68         & —                 & 6/13          \\
            Algerian Arabic(arq)                    & \citep{wang2024multilingual}   + SVM                                                         & 51.07                        & 66.87                        & 24/43                       & 36.54         & 63.38         & 36.37             & 18/26         & 51.07         & 65.35         & 55.75             & 6/14          \\
            Moroccan Arabic(ary)                    & \citep{wang2024multilingual}   + SVM                                                         & 51.88                        & 62.92                        & 20/41                       & —             & —             & —                 & —             & 51.88         & 63.22         & 52.76             & 6/12          \\
            Chinese(chn)                            & ZPoint Large\footnote{\url{https://huggingface.co/iampanda/zpoint_large_embedding_zh}} + SVM & 56.65                        & 70.94                        & 30/41                       & 48.47         & 72.24         & 51.86             & 19/27         & 56.65         & 56.65         & 55.23             & 7/15          \\
            German(deu)                             & \citep{wang2024multilingual}       + SVM                                                     & 60.60                        & 73.99                        & 25/50                       & 54.10         & 76.57         & 56.21             & 18/27         & 60.60         & 73.62         & 59.17             & 6/15          \\
            English(eng)                            & \citep{zhang2025jasperstelladistillationsota}                                                & 73.97                        & 82.30                        & 35/96                       & 68.81         & 84.04         & 64.15             & 28/43         & 73.97         & 82.07         & 65.58             & 4/17          \\
            Spanish(esp)                            & \citep{wang2024multilingual}      + SVM                                                      & 76.19                        & 84.88                        & 26/47                       & 66.70         & 80.80         & 72.59             & 23/29         & 76.19         & 85.00         & 73.29             & 5/15          \\
            Hausa(hau)                              & \citep{dobler-demelo-2023-focus} + SVM                                                       & 63.22                        & 75.07                        & 18/40                       & 58.42         & 77.00         & 39.16             & 15/26         & 63.22         & 73.14         & 51.91             & 4/13          \\
            Hindi(hin)                              & \citep{wang2024multilingual}  + SVM                                                          & 80.32                        & 92.57                        & 33/43                       & —             & —             & —                 & —             & 80.32         & 92.16         & 79.73             & 6/16          \\
            Igbo(ibo)                               & \citep{wang2024multilingual}   + SVM                                                         & 50.93                        & 60.01                        & 14/34                       & —             & —             & —                 & —             & 50.93         & 60.47         & 37.40             & 4/12          \\
            Indonesian(ind)                         & \citep{wang2024multilingual}   + XGB                                                         & —                            & —                            & —                           & —             & —             & —                 & —             & 35.64         & 67.24         & 57.29             & 15/17         \\
            Javanese(jav)                           & \citep{wang2024multilingual}   + XGB                                                         & —                            & —                            & —                           & —             & —             & —                 & —             & 25.62         & 25.62         & 50.47             & 12/13         \\
            Kinyarwanda(kin)                        & \citep{wang2024multilingual}  + SVM                                                          & 51.94                        & 65.74                        & 6/31                        & —             & —             & —                 & —             & 51.94         & 64.59         & 34.36             & 2/11          \\
            Marathi(mar)                            & \citep{wang2024multilingual}   + SVM                                                         & 81.10                        & 90.58                        & 23/42                       & —             & —             & —                 & —             & 81.10         & 90.42         & 77.24             & 6/13          \\
            Oromo(orm)                              & \citep{wang2024multilingual}    + SVM                                                        & 54.31                        & 61.64                        & 11/36                       & —             & —             & —                 & —             & 54.31         & 60.07         & —                 & 3/11          \\
            Nigerian-Pidgin(pcm)                    & \citep{wang2024multilingual}   + SVM                                                         & 53.09                        & 67.40                        & 22/34                       & —             & —             & —                 & —             & 53.09         & 67.40         & 48.67             & 5/10          \\
            Pt\footnote{Portuguese} Brazilian(ptbr) & \citep{souza2020bertimbau} + SVM                                                             & 47.99                        & 68.33                        & 27/42                       & 38.20         & 71.00         & 46.72             & 22/25         & 47.99         & 68.36         & 51.60             & 8/14          \\
            Pt Mozambican(ptmz)                     & \citep{wang2024multilingual}   + SVM                                                         & 50.08                        & 54.77                        & 6/36                        & —             & —             & —                 & —             & 50.08         & 55.54         & 40.44             & 3/14          \\
            Romanian(ron)                           & \citep{wang2024multilingual}     + SVM                                                       & 73.75                        & 79.43                        & 14/43                       & 57.61         & 72.60         & 57.69             & 19/26         & 73.75         & 77.27         & 76.23             & 4/15          \\
            Russian(rus)                            & \citep{snegirev2025russianfocusedembeddersexplorationrumteb}     + SVM                       & 82.42                        & 90.87                        & 33/50                       & 78.41         & 92.54         & 87.66             & 22/30         & 82.42         & 90.62         & 76.97             & 6/17          \\
            Somali(som)                             & \citep{wang2024multilingual}       + SVM                                                     & 48.26                        & 57.65                        & 10/34                       & —             & —             & —                 & —             & 48.26         & 56.66         & —                 & 4/13          \\
            Sundanese(sun)                          & \citep{wang2024multilingual}      + SVM                                                      & 42.48                        & 54.97                        & 19/37                       & —             & —             & —                 & —             & 42.48         & 50.72         & 46.33             & 4/11          \\
            Swahili(swa)                            & \citep{wang2024multilingual}      + SVM                                                      & 29.52                        & 41.47                        & 15/32                       & —             & —             & —                 & —             & 29.52         & 38.43         & 33.27             & 5/14          \\
            Swedish(swe)                            & \citep{wang2024multilingual}     + SVM                                                       & 56.51                        & 62.62                        & 13/40                       & —             & —             & —                 & —             & 56.51         & 64.53         & 51.18             & 6/13          \\
            Tatar(tat)                              & \citep{wang2024multilingual}    + SVM                                                        & 64.32                        & 84.59                        & 18/36                       & —             & —             & —                 & —             & 64.32         & 83.59         & 60.66             & 5/11          \\
            Tigrinya(tir)                           & \citep{wang2024multilingual}    + SVM                                                        & 52.37                        & 59.05                        & 6/34                        & —             & —             & —                 & —             & 52.37         & 55.24         & —                 & 2/10          \\
            Ukrainian(ukr)                          & \citep{sturua2024jinaembeddingsv3multilingualembeddingstask}   + SVM                         & 48.62                        & 72.56                        & 29/40                       & 42.55         & 70.75         & 43.54             & 17/24         & 48.62         & 71.99         & 54.76             & 10/17         \\
            Emakhuwa(vmw)                           & \citep{sturua2024jinaembeddingsv3multilingualembeddingstask}   + SVM                         & 16.81                        & 32.50                        & 5/31                        & —             & —             & —                 & —             & 16.80         & 26.02         & 20.41             & 6/9           \\
            isiXhosa(xho)                           & \citep{wang2024multilingual}     + XGB                                                       & —                            & —                            & —                           & —             & —             & —                 & —             & 16.64         & 44.26         & 30.79             & 6/10          \\
            Yoruba(yor)                             & \citep{wang2024multilingual}     + SVM                                                       & 34.09                        & 46.13                        & 9/32                        & —             & —             & —                 & —             & 34.09         & 46.79         & 27.44             & 4/11          \\
            isiZulu(zul)                            & \citep{wang2024multilingual}     + XGB                                                       & —                            & —                            & —                           & —             & —             & —                 & —             & 16.35         & 16.35         & 22.03             & 8/11          \\
            \hline
        \end{tabular}
    }
\end{table*}


\subsection{Data Splits and Usage}
The dataset\cite{muhammad2025brighterbridginggaphumanannotated} was divided into three subsets: training, development (validation), and testing. Specifically, 80\% of the training dataset was allocated for training, while the remaining 20\% was reserved for validation to facilitate model selection. Once the best-performing model was identified during the validation phase, the entire training and development datasets were combined to retrain the final model. This final model was then evaluated on the test dataset, which was held out during the entire training process to ensure an unbiased assessment of the model's generalization performance. This approach adheres to standard practices in machine learning research to prevent data leakage and ensure robust evaluation \citep{Goodfellow-et-al-2016}.

\subsection{Preprocessing}

Preprocessing of the dataset was performed using the \texttt{clean-text} library. The preprocessing pipeline included standardizing the text by lowercasing all characters, removing special characters and punctuation, and normalizing tokens through contraction expansion and the elimination of redundant whitespace. These steps ensured the data was clean and consistent across all subsets. Details of the preprocessing pipeline are provided in the supplementary material (Appendix A). Preprocessing was applied consistently to the training, validation, and test datasets to avoid introducing biases or inconsistencies. Such preprocessing steps have been shown to improve the performance of natural language processing (NLP) models by reducing noise and simplifying the input representations \citep{Zhang2020DataPrep}.

\subsection{Hyperparameter Tuning}

Hyperparameter tuning was conducted using \texttt{Optuna}, an advanced framework for automated hyperparameter optimization \citep{Akiba2019Optuna}. The hyperparameters tuned included the learning rate, batch size, dropout rate, and the number of transformer layers to fine-tune. A Bayesian optimization strategy was employed, balancing exploration of the hyperparameter space and exploitation of promising configurations. The performance of each hyperparameter configuration was evaluated on the validation set, and the best configuration was selected based on the performance metric of interest.

\subsection{Model Training and Optimization}

The model training process relied on Binary Cross-Entropy (BCE) as the loss function for Track A and C, and Cross-Entropy for Track B due to its suitability for classification tasks. Given the unbalanced dataset, a weighted loss approach was employed to ensure that the model adequately learned from all classes. Optimization was performed using the \texttt{AdamW} optimizer, which improves upon the standard Adam optimizer by decoupling weight decay and learning rate updates \citep{Loshchilov2019AdamW}. To further enhance training stability and convergence, a cosine annealing learning rate scheduler with restarts was employed. This scheduling approach helped adaptively reduce the learning rate over time, facilitating better exploration of the loss landscape and improving generalization. The model was trained for a fixed number of epochs, and early stopping was used to terminate training if the validation performance plateaued, thus avoiding overfitting.

\subsection{Tools and Libraries}

The implementation of the experiments utilized several state-of-the-art tools and libraries. The deep learning models were implemented and trained using \texttt{PyTorch}, which provides a flexible and high-performance framework for machine learning research \citep{Paszke2019PyTorch}. For data manipulation and evaluation metrics, \texttt{Scikit-Learn} was employed \citep{Pedregosa2011ScikitLearn}. Gradient boosting models were benchmarked using \texttt{XGBoost}, which is widely regarded for its efficiency and scalability \citep{chen2016xgboost}. Pre-trained transformer models were fine-tuned using \texttt{Hugging Face Transformers}, a library that offers robust implementations of transformer-based architectures \citep{Wolf2019HuggingFace}. These tools and libraries are well-regarded in the machine learning community and were chosen for their reliability and performance.

\subsection{Computational Resources}

All experiments were run on a Kaggle Tesla P100 GPU, which provided the necessary computational power to efficiently train and evaluate the models. The use of a Tesla P100 allowed for accelerated processing, making it feasible to leverage complex models and extensive hyperparameter tuning within a reasonable timeframe. This setup ensures that the experiments are reproducible and can be replicated by other researchers with access to similar hardware.

\subsection{Postprocessing}

Postprocessing involved thresholding the model's output probabilities to convert them into binary class predictions. A default threshold of 0.5 was used, but sensitivity analyses were conducted to evaluate the impact of varying the threshold. This ensured that the model's performance was robust across different operating points and provided interpretable results for downstream tasks. Minimal additional postprocessing was applied to maintain the integrity of the model's predictions.

% \subsection{Concluding Remarks on Setup}

% The experimental setup was designed to maximize the reliability, reproducibility, and validity of the results. By adhering to established best practices in data splitting, preprocessing, and hyperparameter tuning, and by leveraging state-of-the-art tools and optimization strategies, the experimental methodology ensures a robust evaluation of the model's performance. Furthermore, the combination of advanced optimization techniques, careful preprocessing, and systematic model selection contributes to the study's alignment with the broader goals of advancing research in machine learning. This setup not only facilitates a fair comparison with existing methods but also provides a strong foundation for further exploration and improvement of the proposed approach.